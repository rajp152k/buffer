
@misc{sheth_neurosymbolic_2023,
	title = {Neurosymbolic {AI} -- {Why}, {What}, and {How}},
	url = {http://arxiv.org/abs/2305.00813},
	doi = {10.48550/arXiv.2305.00813},
	abstract = {Humans interact with the environment using a combination of perception - transforming sensory inputs from their environment into symbols, and cognition - mapping symbols to knowledge about the environment for supporting abstraction, reasoning by analogy, and long-term planning. Human perception-inspired machine perception, in the context of AI, refers to large-scale pattern recognition from raw data using neural networks trained using self-supervised learning objectives such as next-word prediction or object recognition. On the other hand, machine cognition encompasses more complex computations, such as using knowledge of the environment to guide reasoning, analogy, and long-term planning. Humans can also control and explain their cognitive functions. This seems to require the retention of symbolic mappings from perception outputs to knowledge about their environment. For example, humans can follow and explain the guidelines and safety constraints driving their decision-making in safety-critical applications such as healthcare, criminal justice, and autonomous driving. This article introduces the rapidly emerging paradigm of Neurosymbolic AI combines neural networks and knowledge-guided symbolic approaches to create more capable and flexible AI systems. These systems have immense potential to advance both algorithm-level (e.g., abstraction, analogy, reasoning) and application-level (e.g., explainable and safety-constrained decision-making) capabilities of AI systems.},
	urldate = {2024-06-17},
	publisher = {arXiv},
	author = {Sheth, Amit and Roy, Kaushik and Gaur, Manas},
	month = may,
	year = {2023},
	note = {arXiv:2305.00813 [cs]},
	keywords = {Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:/home/rp152k/Zotero/storage/XKHCL4IW/Sheth et al. - 2023 - Neurosymbolic AI -- Why, What, and How.pdf:application/pdf;arXiv.org Snapshot:/home/rp152k/Zotero/storage/JWNHUYEJ/2305.html:text/html},
}

@misc{garcez_neurosymbolic_2020,
	title = {Neurosymbolic {AI}: {The} 3rd {Wave}},
	shorttitle = {Neurosymbolic {AI}},
	url = {http://arxiv.org/abs/2012.05876},
	doi = {10.48550/arXiv.2012.05876},
	abstract = {Current advances in Artificial Intelligence (AI) and Machine Learning (ML) have achieved unprecedented impact across research communities and industry. Nevertheless, concerns about trust, safety, interpretability and accountability of AI were raised by influential thinkers. Many have identified the need for well-founded knowledge representation and reasoning to be integrated with deep learning and for sound explainability. Neural-symbolic computing has been an active area of research for many years seeking to bring together robust learning in neural networks with reasoning and explainability via symbolic representations for network models. In this paper, we relate recent and early research results in neurosymbolic AI with the objective of identifying the key ingredients of the next wave of AI systems. We focus on research that integrates in a principled way neural network-based learning with symbolic knowledge representation and logical reasoning. The insights provided by 20 years of neural-symbolic computing are shown to shed new light onto the increasingly prominent role of trust, safety, interpretability and accountability of AI. We also identify promising directions and challenges for the next decade of AI research from the perspective of neural-symbolic systems.},
	urldate = {2024-06-17},
	publisher = {arXiv},
	author = {Garcez, Artur d'Avila and Lamb, Luis C.},
	month = dec,
	year = {2020},
	note = {arXiv:2012.05876 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, I.2.4, I.2.6},
	file = {arXiv Fulltext PDF:/home/rp152k/Zotero/storage/62RGF8PB/Garcez and Lamb - 2020 - Neurosymbolic AI The 3rd Wave.pdf:application/pdf;arXiv.org Snapshot:/home/rp152k/Zotero/storage/3QTEEPKH/2012.html:text/html},
}

@article{hitzler_neural-symbolic_2020,
	title = {Neural-symbolic integration and the {Semantic} {Web}},
	volume = {11},
	issn = {22104968, 15700844},
	url = {https://www.medra.org/servlet/aliasResolver?alias=iospress&doi=10.3233/SW-190368},
	doi = {10.3233/SW-190368},
	language = {en},
	number = {1},
	urldate = {2024-06-17},
	journal = {Semantic Web},
	author = {Hitzler, Pascal and Bianchi, Federico and Ebrahimi, Monireh and Sarker, Md Kamruzzaman},
	editor = {Janowicz, Krzysztof},
	month = jan,
	year = {2020},
	pages = {3--11},
	file = {Hitzler et al. - 2020 - Neural-symbolic integration and the Semantic Web.pdf:/home/rp152k/Zotero/storage/TXQTYC6N/Hitzler et al. - 2020 - Neural-symbolic integration and the Semantic Web.pdf:application/pdf},
}

@misc{bottou_machine_2011,
	title = {From {Machine} {Learning} to {Machine} {Reasoning}},
	url = {http://arxiv.org/abs/1102.1808},
	doi = {10.48550/arXiv.1102.1808},
	abstract = {A plausible definition of "reasoning" could be "algebraically manipulating previously acquired knowledge in order to answer a new question". This definition covers first-order logical inference or probabilistic inference. It also includes much simpler manipulations commonly used to build large learning systems. For instance, we can build an optical character recognition system by first training a character segmenter, an isolated character recognizer, and a language model, using appropriate labeled training sets. Adequately concatenating these modules and fine tuning the resulting system can be viewed as an algebraic operation in a space of models. The resulting model answers a new question, that is, converting the image of a text page into a computer readable text. This observation suggests a conceptual continuity between algebraically rich inference systems, such as logical or probabilistic inference, and simple manipulations, such as the mere concatenation of trainable learning systems. Therefore, instead of trying to bridge the gap between machine learning systems and sophisticated "all-purpose" inference mechanisms, we can instead algebraically enrich the set of manipulations applicable to training systems, and build reasoning capabilities from the ground up.},
	urldate = {2024-06-17},
	publisher = {arXiv},
	author = {Bottou, Leon},
	month = feb,
	year = {2011},
	note = {arXiv:1102.1808 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/home/rp152k/Zotero/storage/MJ6VVSW2/Bottou - 2011 - From Machine Learning to Machine Reasoning.pdf:application/pdf;arXiv.org Snapshot:/home/rp152k/Zotero/storage/GYZQ2VD2/1102.html:text/html},
}

@misc{de_raedt_statistical_2020,
	title = {From {Statistical} {Relational} to {Neuro}-{Symbolic} {Artificial} {Intelligence}},
	url = {http://arxiv.org/abs/2003.08316},
	doi = {10.48550/arXiv.2003.08316},
	abstract = {Neuro-symbolic and statistical relational artificial intelligence both integrate frameworks for learning with logical reasoning. This survey identifies several parallels across seven different dimensions between these two fields. These cannot only be used to characterize and position neuro-symbolic artificial intelligence approaches but also to identify a number of directions for further research.},
	urldate = {2024-06-17},
	publisher = {arXiv},
	author = {De Raedt, Luc and Dumančić, Sebastijan and Manhaeve, Robin and Marra, Giuseppe},
	month = mar,
	year = {2020},
	note = {arXiv:2003.08316 [cs]},
	keywords = {Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:/home/rp152k/Zotero/storage/U4FGFF77/De Raedt et al. - 2020 - From Statistical Relational to Neuro-Symbolic Arti.pdf:application/pdf;arXiv.org Snapshot:/home/rp152k/Zotero/storage/8IVPLMHU/2003.html:text/html},
}

@misc{noauthor_neuro-symbolic_nodate,
	title = {Neuro-{Symbolic} {Artificial} {Intelligence} - workshops},
	url = {https://people.cs.ksu.edu/~hitzler/nesy/},
	urldate = {2024-06-17},
	file = {Neuro-Symbolic Artificial Intelligence:/home/rp152k/Zotero/storage/63DNPJFW/nesy.html:text/html},
}

@misc{lamb_graph_2021,
	title = {Graph {Neural} {Networks} {Meet} {Neural}-{Symbolic} {Computing}: {A} {Survey} and {Perspective}},
	shorttitle = {Graph {Neural} {Networks} {Meet} {Neural}-{Symbolic} {Computing}},
	url = {http://arxiv.org/abs/2003.00330},
	doi = {10.48550/arXiv.2003.00330},
	abstract = {Neural-symbolic computing has now become the subject of interest of both academic and industry research laboratories. Graph Neural Networks (GNN) have been widely used in relational and symbolic domains, with widespread application of GNNs in combinatorial optimization, constraint satisfaction, relational reasoning and other scientific domains. The need for improved explainability, interpretability and trust of AI systems in general demands principled methodologies, as suggested by neural-symbolic computing. In this paper, we review the state-of-the-art on the use of GNNs as a model of neural-symbolic computing. This includes the application of GNNs in several domains as well as its relationship to current developments in neural-symbolic computing.},
	urldate = {2024-06-17},
	publisher = {arXiv},
	author = {Lamb, Luis C. and Garcez, Artur and Gori, Marco and Prates, Marcelo and Avelar, Pedro and Vardi, Moshe},
	month = jun,
	year = {2021},
	note = {arXiv:2003.00330 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Logic in Computer Science, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/home/rp152k/Zotero/storage/2SP4FU4E/Lamb et al. - 2021 - Graph Neural Networks Meet Neural-Symbolic Computi.pdf:application/pdf;arXiv.org Snapshot:/home/rp152k/Zotero/storage/URE2YKP9/2003.html:text/html},
}

@misc{battaglia_relational_2018,
	title = {Relational inductive biases, deep learning, and graph networks},
	url = {http://arxiv.org/abs/1806.01261},
	doi = {10.48550/arXiv.1806.01261},
	abstract = {Artificial intelligence (AI) has undergone a renaissance recently, making major progress in key domains such as vision, language, control, and decision-making. This has been due, in part, to cheap data and cheap compute resources, which have fit the natural strengths of deep learning. However, many defining characteristics of human intelligence, which developed under much different pressures, remain out of reach for current approaches. In particular, generalizing beyond one's experiences--a hallmark of human intelligence from infancy--remains a formidable challenge for modern AI. The following is part position paper, part review, and part unification. We argue that combinatorial generalization must be a top priority for AI to achieve human-like abilities, and that structured representations and computations are key to realizing this objective. Just as biology uses nature and nurture cooperatively, we reject the false choice between "hand-engineering" and "end-to-end" learning, and instead advocate for an approach which benefits from their complementary strengths. We explore how using relational inductive biases within deep learning architectures can facilitate learning about entities, relations, and rules for composing them. We present a new building block for the AI toolkit with a strong relational inductive bias--the graph network--which generalizes and extends various approaches for neural networks that operate on graphs, and provides a straightforward interface for manipulating structured knowledge and producing structured behaviors. We discuss how graph networks can support relational reasoning and combinatorial generalization, laying the foundation for more sophisticated, interpretable, and flexible patterns of reasoning. As a companion to this paper, we have released an open-source software library for building graph networks, with demonstrations of how to use them in practice.},
	urldate = {2024-06-17},
	publisher = {arXiv},
	author = {Battaglia, Peter W. and Hamrick, Jessica B. and Bapst, Victor and Sanchez-Gonzalez, Alvaro and Zambaldi, Vinicius and Malinowski, Mateusz and Tacchetti, Andrea and Raposo, David and Santoro, Adam and Faulkner, Ryan and Gulcehre, Caglar and Song, Francis and Ballard, Andrew and Gilmer, Justin and Dahl, George and Vaswani, Ashish and Allen, Kelsey and Nash, Charles and Langston, Victoria and Dyer, Chris and Heess, Nicolas and Wierstra, Daan and Kohli, Pushmeet and Botvinick, Matt and Vinyals, Oriol and Li, Yujia and Pascanu, Razvan},
	month = oct,
	year = {2018},
	note = {arXiv:1806.01261 [cs, stat]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/home/rp152k/Zotero/storage/S6TRQ9KR/Battaglia et al. - 2018 - Relational inductive biases, deep learning, and gr.pdf:application/pdf;arXiv.org Snapshot:/home/rp152k/Zotero/storage/JRHFDMQ4/1806.html:text/html},
}

@misc{yi_neural-symbolic_2019,
	title = {Neural-{Symbolic} {VQA}: {Disentangling} {Reasoning} from {Vision} and {Language} {Understanding}},
	shorttitle = {Neural-{Symbolic} {VQA}},
	url = {http://arxiv.org/abs/1810.02338},
	doi = {10.48550/arXiv.1810.02338},
	abstract = {We marry two powerful ideas: deep representation learning for visual recognition and language understanding, and symbolic program execution for reasoning. Our neural-symbolic visual question answering (NS-VQA) system first recovers a structural scene representation from the image and a program trace from the question. It then executes the program on the scene representation to obtain an answer. Incorporating symbolic structure as prior knowledge offers three unique advantages. First, executing programs on a symbolic space is more robust to long program traces; our model can solve complex reasoning tasks better, achieving an accuracy of 99.8\% on the CLEVR dataset. Second, the model is more data- and memory-efficient: it performs well after learning on a small number of training data; it can also encode an image into a compact representation, requiring less storage than existing methods for offline question answering. Third, symbolic program execution offers full transparency to the reasoning process; we are thus able to interpret and diagnose each execution step.},
	urldate = {2024-06-17},
	publisher = {arXiv},
	author = {Yi, Kexin and Wu, Jiajun and Gan, Chuang and Torralba, Antonio and Kohli, Pushmeet and Tenenbaum, Joshua B.},
	month = jan,
	year = {2019},
	note = {arXiv:1810.02338 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/home/rp152k/Zotero/storage/WQ8SJ528/Yi et al. - 2019 - Neural-Symbolic VQA Disentangling Reasoning from .pdf:application/pdf;arXiv.org Snapshot:/home/rp152k/Zotero/storage/9CIDLP4A/1810.html:text/html},
}

@misc{vaswani_attention_2023,
	title = {Attention {Is} {All} {You} {Need}},
	url = {http://arxiv.org/abs/1706.03762},
	doi = {10.48550/arXiv.1706.03762},
	abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
	urldate = {2024-06-19},
	publisher = {arXiv},
	author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
	month = aug,
	year = {2023},
	note = {arXiv:1706.03762 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/home/rp152k/Zotero/storage/335MFALG/Vaswani et al. - 2023 - Attention Is All You Need.pdf:application/pdf;arXiv.org Snapshot:/home/rp152k/Zotero/storage/KR7F69YM/1706.html:text/html},
}

@misc{zhang_new_2023,
	title = {A {New} {Information} {Theory} of {Certainty} for {Machine} {Learning}},
	url = {http://arxiv.org/abs/2304.12833},
	doi = {10.48550/arXiv.2304.12833},
	abstract = {Claude Shannon coined entropy to quantify the uncertainty of a random distribution for communication coding theory. We observe that the uncertainty nature of entropy also limits its direct usage in mathematical modeling. Therefore we propose a new concept troenpy,as the canonical dual of entropy, to quantify the certainty of the underlying distribution. We demonstrate two applications in machine learning. The first is for the classical document classification, we develop a troenpy based weighting scheme to leverage the document class label. The second is a self-troenpy weighting scheme for sequential data and show that it can be easily included in neural network based language models and achieve dramatic perplexity reduction. We also define quantum troenpy as the dual of the Von Neumann entropy to quantify the certainty of quantum systems.},
	urldate = {2024-07-07},
	publisher = {arXiv},
	author = {Zhang, Arthur Jun},
	month = apr,
	year = {2023},
	note = {arXiv:2304.12833 [cs, math]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Information Retrieval, Computer Science - Information Theory, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/home/rp152k/Zotero/storage/RREK5VQY/Zhang - 2023 - A New Information Theory of Certainty for Machine .pdf:application/pdf;arXiv.org Snapshot:/home/rp152k/Zotero/storage/MS3Y5DB3/2304.html:text/html},
}

@misc{qiao_we-math_2024,
	title = {We-{Math}: {Does} {Your} {Large} {Multimodal} {Model} {Achieve} {Human}-like {Mathematical} {Reasoning}?},
	shorttitle = {We-{Math}},
	url = {http://arxiv.org/abs/2407.01284},
	doi = {10.48550/arXiv.2407.01284},
	abstract = {Visual mathematical reasoning, as a fundamental visual reasoning ability, has received widespread attention from the Large Multimodal Models (LMMs) community. Existing benchmarks, such as MathVista and MathVerse, focus more on the result-oriented performance but neglect the underlying principles in knowledge acquisition and generalization. Inspired by human-like mathematical reasoning, we introduce WE-MATH, the first benchmark specifically designed to explore the problem-solving principles beyond end-to-end performance. We meticulously collect and categorize 6.5K visual math problems, spanning 67 hierarchical knowledge concepts and five layers of knowledge granularity. We decompose composite problems into sub-problems according to the required knowledge concepts and introduce a novel four-dimensional metric, namely Insufficient Knowledge (IK), Inadequate Generalization (IG), Complete Mastery (CM), and Rote Memorization (RM), to hierarchically assess inherent issues in LMMs' reasoning process. With WE-MATH, we conduct a thorough evaluation of existing LMMs in visual mathematical reasoning and reveal a negative correlation between solving steps and problem-specific performance. We confirm the IK issue of LMMs can be effectively improved via knowledge augmentation strategies. More notably, the primary challenge of GPT-4o has significantly transitioned from IK to IG, establishing it as the first LMM advancing towards the knowledge generalization stage. In contrast, other LMMs exhibit a marked inclination towards Rote Memorization - they correctly solve composite problems involving multiple knowledge concepts yet fail to answer sub-problems. We anticipate that WE-MATH will open new pathways for advancements in visual mathematical reasoning for LMMs. The WE-MATH data and evaluation code are available at https://github.com/We-Math/We-Math.},
	urldate = {2024-07-07},
	publisher = {arXiv},
	author = {Qiao, Runqi and Tan, Qiuna and Dong, Guanting and Wu, Minhui and Sun, Chong and Song, Xiaoshuai and GongQue, Zhuoma and Lei, Shanglin and Wei, Zhe and Zhang, Miaoxuan and Qiao, Runfeng and Zhang, Yifan and Zong, Xiao and Xu, Yida and Diao, Muxi and Bao, Zhimin and Li, Chen and Zhang, Honggang},
	month = jul,
	year = {2024},
	note = {arXiv:2407.01284 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Symbolic Computation},
	file = {arXiv Fulltext PDF:/home/rp152k/Zotero/storage/IU5B49TG/Qiao et al. - 2024 - We-Math Does Your Large Multimodal Model Achieve .pdf:application/pdf;arXiv.org Snapshot:/home/rp152k/Zotero/storage/NSFPNXAV/2407.html:text/html},
}

@misc{petryk_importance_2024,
	title = {On the {Importance} of {Reproducibility} of {Experimental} {Results} {Especially} in the {Domain} of {Security}},
	url = {http://arxiv.org/abs/2407.06760},
	doi = {10.1109/MECO62516.2024.10577919},
	abstract = {Security especially in the fields of IoT, industrial automation and critical infrastructure is paramount nowadays and a hot research topic. In order to ensure confidence in research results they need to be reproducible. In the past we reported [18] that in many publications important information such as details about the equipment used are missing. In this paper we report on our own experiments that we run to verify the parameters reported in the datasheets that came along with our experimental equipment. Our results show that there are significant discrepancies between the datasheets and the real world data. These deviations concern accuracy of positions, movements, duration of laser shots etc. In order to improve reproducibility of results we therefore argue on the one hand that research groups verify the data given in datasheets of equipment they use and on the other hand that they provide measurement set-up parameters in globally accepted units such as cm, seconds, etc.},
	urldate = {2024-07-10},
	author = {Petryk, Dmytro and Kabin, Ievgen and Langendörfer, Peter and Dyka, Zoya},
	month = jul,
	year = {2024},
	note = {arXiv:2407.06760 [cs]},
	keywords = {Computer Science - Cryptography and Security, Computer Science - Hardware Architecture},
	file = {arXiv Fulltext PDF:/home/rp152k/Zotero/storage/HJHIXPCN/Petryk et al. - 2024 - On the Importance of Reproducibility of Experiment.pdf:application/pdf;arXiv.org Snapshot:/home/rp152k/Zotero/storage/6BBHTLMP/2407.html:text/html},
}

@misc{kalwarowskyj_parallel_2023,
	title = {Parallel {Neural} {Networks} in {Golang}},
	url = {http://arxiv.org/abs/2304.09590},
	doi = {10.48550/arXiv.2304.09590},
	abstract = {This paper describes the design and implementation of parallel neural networks (PNNs) with the novel programming language Golang. We follow in our approach the classical Single-Program Multiple-Data (SPMD) model where a PNN is composed of several sequential neural networks, which are trained with a proportional share of the training dataset. We used for this purpose the MNIST dataset, which contains binary images of handwritten digits. Our analysis focusses on different activation functions and optimizations in the form of stochastic gradients and initialization of weights and biases. We conduct a thorough performance analysis, where network configurations and different performance factors are analyzed and interpreted. Golang and its inherent parallelization support proved very well for parallel neural network simulation by considerable decreased processing times compared to sequential variants.},
	urldate = {2024-08-23},
	publisher = {arXiv},
	author = {Kalwarowskyj, Daniela and Schikuta, Erich},
	month = apr,
	year = {2023},
	note = {arXiv:2304.09590 [cs]},
	keywords = {68T07, Computer Science - Distributed, Parallel, and Cluster Computing, Computer Science - Neural and Evolutionary Computing, I.2},
	file = {arXiv Fulltext PDF:/home/rp152k/Zotero/storage/HBBHLAUI/Kalwarowskyj and Schikuta - 2023 - Parallel Neural Networks in Golang.pdf:application/pdf;arXiv.org Snapshot:/home/rp152k/Zotero/storage/V6W59QV2/2304.html:text/html},
}

@misc{ueno_migrating_2024,
	title = {Migrating {Existing} {Container} {Workload} to {Kubernetes} -- {LLM} {Based} {Approach} and {Evaluation}},
	url = {http://arxiv.org/abs/2408.11428},
	doi = {10.48550/arXiv.2408.11428},
	abstract = {Although Kubernetes has become a widespread open-source system that automates the management of containerized applications, its complexity can be a significant barrier, particularly for application developers unfamiliar with it. One approach employs large language models (LLMs) to assist developers in generating Kubernetes manifests; however it is currently impossible to determine whether the output satisfies given specifications and is comprehensible. In this study, we proposed a benchmarking method for evaluating the effectiveness of LLMs in synthesizing manifests, using the Compose specification -- a standard widely adopted by application developers -- as input. The proposed benchmarking method revealed that LLMs generally produce accurate results that compensate for simple specification gaps. However, we also observed that inline comments for readability were often omitted, and completion accuracy was low for atypical inputs with unclear intentions.},
	urldate = {2024-08-23},
	publisher = {arXiv},
	author = {Ueno, Masaru and Uchiumi, Tetsuya},
	month = aug,
	year = {2024},
	note = {arXiv:2408.11428 [cs]},
	keywords = {Computer Science - Software Engineering},
	file = {arXiv Fulltext PDF:/home/rp152k/Zotero/storage/3M5HNIV6/Ueno and Uchiumi - 2024 - Migrating Existing Container Workload to Kubernete.pdf:application/pdf;arXiv.org Snapshot:/home/rp152k/Zotero/storage/ATBNY8SK/2408.html:text/html},
}

@article{gupta_columnar_2021,
	title = {Columnar storage and list-based processing for graph database management systems},
	volume = {14},
	issn = {2150-8097},
	url = {https://dl.acm.org/doi/10.14778/3476249.3476297},
	doi = {10.14778/3476249.3476297},
	abstract = {We revisit column-oriented storage and query processing techniques in the context of contemporary graph database management systems (GDBMSs). Similar to column-oriented RDBMSs, GDBMSs support read-heavy analytical workloads that however have fundamentally different data access patterns than traditional analytical workloads. We first derive a set of desiderata for optimizing storage and query processors of GDBMS based on their access patterns. We then present the design of columnar storage, compression, and query processing techniques based on these desiderata. In addition to showing direct integration of existing techniques from columnar RDBMSs, we also propose novel ones that are optimized for GDBMSs. These include a novel list-based query processor, which avoids expensive data copies of traditional block-based processors under many-to-many joins, a new data structure we call singleindexed edge property pages and an accompanying edge ID scheme, and a new application of Jacobson’s bit vector index for compressing NULL values and empty lists. We integrated our techniques into the GraphflowDB in-memory GDBMS. Through extensive experiments, we demonstrate the scalability and query performance benefits of our techniques.},
	language = {en},
	number = {11},
	urldate = {2024-09-07},
	journal = {Proceedings of the VLDB Endowment},
	author = {Gupta, Pranjal and Mhedhbi, Amine and Salihoglu, Semih},
	month = jul,
	year = {2021},
	pages = {2491--2504},
	file = {Gupta et al. - 2021 - Columnar storage and list-based processing for gra.pdf:/home/rp152k/Zotero/storage/EAE8LYB9/Gupta et al. - 2021 - Columnar storage and list-based processing for gra.pdf:application/pdf},
}

@misc{noauthor_rabbit_nodate,
	title = {‪{Rabbit}: {Efficient} {Comparison} for {Secure} {Multi}-{Party} {Computation}‬},
	shorttitle = {‪{Rabbit}},
	url = {https://scholar.google.com/citations?view_op=view_citation&hl=en&user=l21zXnAAAAAJ&sortby=pubdate&citation_for_view=l21zXnAAAAAJ:Zph67rFs4hoC},
	abstract = {‪E Makri, D Rotaru, F Vercauteren, S Wagh, 2021‬ - ‪Cited by 5‬},
	urldate = {2021-11-20},
}

@misc{yahuda_lindell_primer_nodate,
	title = {A {Primer} in {Secure} {Multiparty} {Computation}},
	url = {https://www.unboundtech.com/wp-content/uploads/2020/09/Unbound_Tech_A_Primer_in_Secure_Multiparty_Computation_MPC.pdf},
	author = {Yahuda, Lindell},
	file = {Yahuda, Lindell - A_Primer_in_Secure_Multiparty_Computation_MPC.pdf:/home/rp152k/Zotero/storage/KWTBZFCC/Yahuda, Lindell - A_Primer_in_Secure_Multiparty_Computation_MPC.pdf:application/pdf},
}

@misc{noauthor_cryptology_nodate,
	title = {Cryptology {ePrint} {Archive}: {Report} 2020/300 - {Secure} {Multiparty} {Computation} ({MPC})},
	url = {https://eprint.iacr.org/2020/300},
	urldate = {2021-11-19},
	file = {Cryptology ePrint Archive\: Report 2020/300 - Secure Multiparty Computation (MPC):/home/rp152k/Zotero/storage/WIH5PDTM/300.html:text/html},
}

@article{viand_sok_2021,
	title = {{SoK}: {Fully} {Homomorphic} {Encryption} {Compilers}},
	shorttitle = {{SoK}},
	url = {http://arxiv.org/abs/2101.07078},
	abstract = {Fully Homomorphic Encryption (FHE) allows a third party to perform arbitrary computations on encrypted data, learning neither the inputs nor the computation results. Hence, it provides resilience in situations where computations are carried out by an untrusted or potentially compromised party. This powerful concept was first conceived by Rivest et al. in the 1970s. However, it remained unrealized until Craig Gentry presented the first feasible FHE scheme in 2009. The advent of the massive collection of sensitive data in cloud services, coupled with a plague of data breaches, moved highly regulated businesses to increasingly demand confidential and secure computing solutions. This demand, in turn, has led to a recent surge in the development of FHE tools. To understand the landscape of recent FHE tool developments, we conduct an extensive survey and experimental evaluation to explore the current state of the art and identify areas for future development. In this paper, we survey, evaluate, and systematize FHE tools and compilers. We perform experiments to evaluate these tools' performance and usability aspects on a variety of applications. We conclude with recommendations for developers intending to develop FHE-based applications and a discussion on future directions for FHE tools development.},
	urldate = {2021-11-17},
	journal = {arXiv:2101.07078 [cs]},
	author = {Viand, Alexander and Jattke, Patrick and Hithnawi, Anwar},
	month = jan,
	year = {2021},
	note = {arXiv: 2101.07078
version: 1},
	keywords = {Computer Science - Cryptography and Security},
	file = {arXiv Fulltext PDF:/home/rp152k/Zotero/storage/N2LQML23/Viand et al. - 2021 - SoK Fully Homomorphic Encryption Compilers.pdf:application/pdf;arXiv.org Snapshot:/home/rp152k/Zotero/storage/TKVA2A2C/2101.html:text/html},
}

@article{nguyen_leep_2020,
	title = {{LEEP}: {A} {New} {Measure} to {Evaluate} {Transferability} of {Learned} {Representations}},
	shorttitle = {{LEEP}},
	url = {http://arxiv.org/abs/2002.12462},
	abstract = {We introduce a new measure to evaluate the transferability of representations learned by classifiers. Our measure, the Log Expected Empirical Prediction (LEEP), is simple and easy to compute: when given a classifier trained on a source data set, it only requires running the target data set through this classifier once. We analyze the properties of LEEP theoretically and demonstrate its effectiveness empirically. Our analysis shows that LEEP can predict the performance and convergence speed of both transfer and meta-transfer learning methods, even for small or imbalanced data. Moreover, LEEP outperforms recently proposed transferability measures such as negative conditional entropy and H scores. Notably, when transferring from ImageNet to CIFAR100, LEEP can achieve up to 30\% improvement compared to the best competing method in terms of the correlations with actual transfer accuracy.},
	urldate = {2021-10-01},
	journal = {arXiv:2002.12462 [cs, stat]},
	author = {Nguyen, Cuong V. and Hassner, Tal and Seeger, Matthias and Archambeau, Cedric},
	month = aug,
	year = {2020},
	note = {arXiv: 2002.12462},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/home/rp152k/Zotero/storage/BVKGGYNR/Nguyen et al. - 2020 - LEEP A New Measure to Evaluate Transferability of.pdf:application/pdf;arXiv.org Snapshot:/home/rp152k/Zotero/storage/AK7NKYD8/2002.html:text/html},
}

@article{dalskov_secure_2020,
	title = {Secure {Evaluation} of {Quantized} {Neural} {Networks}},
	volume = {2020},
	issn = {2299-0984},
	url = {http://arxiv.org/abs/1910.12435},
	doi = {10.2478/popets-2020-0077},
	abstract = {We investigate two questions in this paper: First, we ask to what extent "MPC friendly" models are already supported by major Machine Learning frameworks such as TensorFlow or PyTorch. Prior works provide protocols that only work on fixed-point integers and specialized activation functions, two aspects that are not supported by popular Machine Learning frameworks, and the need for these specialized model representations means that it is hard, and often impossible, to use e.g., TensorFlow to design, train and test models that later have to be evaluated securely. Second, we ask to what extent the functionality for evaluating Neural Networks already exists in general-purpose MPC frameworks. These frameworks have received more scrutiny, are better documented and supported on more platforms. Furthermore, they are typically flexible in terms of the threat model they support. In contrast, most secure evaluation protocols in the literature are targeted to a specific threat model and their implementations are only a "proof-of-concept", making it very hard for their adoption in practice. We answer both of the above questions in a positive way: We observe that the quantization techniques supported by both TensorFlow, PyTorch and MXNet can provide models in a representation that can be evaluated securely; and moreover, that this evaluation can be performed by a general purpose MPC framework. We perform extensive benchmarks to understand the exact trade-offs between different corruption models, network sizes and efficiency. These experiments provide an interesting insight into cost between active and passive security, as well as honest and dishonest majority. Our work shows then that the separating line between existing ML frameworks and existing MPC protocols may be narrower than implicitly suggested by previous works.},
	number = {4},
	urldate = {2021-09-12},
	journal = {Proceedings on Privacy Enhancing Technologies},
	author = {Dalskov, Anders and Escudero, Daniel and Keller, Marcel},
	month = oct,
	year = {2020},
	note = {arXiv: 1910.12435},
	keywords = {Computer Science - Machine Learning, Computer Science - Cryptography and Security},
	pages = {355--375},
	file = {arXiv Fulltext PDF:/home/rp152k/Zotero/storage/GJLYQGMX/Dalskov et al. - 2020 - Secure Evaluation of Quantized Neural Networks.pdf:application/pdf;arXiv.org Snapshot:/home/rp152k/Zotero/storage/827RD3SA/1910.html:text/html},
}

@article{hennig_how_2013,
	title = {How to find an appropriate clustering for mixed-type variables with application to socio-economic stratification: {How} to {Find} an {Appropriate} {Clustering}},
	volume = {62},
	issn = {00359254},
	shorttitle = {How to find an appropriate clustering for mixed-type variables with application to socio-economic stratification},
	url = {http://doi.wiley.com/10.1111/j.1467-9876.2012.01066.x},
	doi = {10.1111/j.1467-9876.2012.01066.x},
	abstract = {Data with mixed-type (metric–ordinal–nominal) variables are typical for social stratiﬁcation, i.e. partitioning a population into social classes. Approaches to cluster such data are compared, namely a latent class mixture model assuming local independence and dissimilarity-based methods such as k -medoids. The design of an appropriate dissimilarity measure and the estimation of the number of clusters are discussed as well, comparing the Bayesian information criterion with dissimilarity-based criteria. The comparison is based on a philosophy of cluster analysis that connects the problem of a choice of a suitable clustering method closely to the application by considering direct interpretations of the implications of the methodology. The application of this philosophy to economic data from the 2007 US Survey of Consumer Finances demonstrates techniques and decisions required to obtain an interpretable clustering. The clustering is shown to be signiﬁcantly more structured than a suitable null model. One result is that the data-based strata are not as strongly connected to occupation categories as is often assumed in the literature.},
	language = {en},
	number = {3},
	urldate = {2021-06-30},
	journal = {Journal of the Royal Statistical Society: Series C (Applied Statistics)},
	author = {Hennig, Christian and Liao, Tim F.},
	month = may,
	year = {2013},
	pages = {309--369},
	file = {Hennig and Liao - 2013 - How to find an appropriate clustering for mixed-ty.pdf:/home/rp152k/Zotero/storage/CNT4ZUFC/Hennig and Liao - 2013 - How to find an appropriate clustering for mixed-ty.pdf:application/pdf},
}

@misc{noauthor_23_nodate,
	title = {2.3. {Clustering} — scikit-learn 0.24.2 documentation},
	url = {https://scikit-learn.org/stable/modules/clustering.html#clustering-performance-evaluation},
	urldate = {2021-06-29},
}

@article{gehrke_rainforest_nodate,
	title = {{RainForest} - a {Framework} for {Fast} {Decision} {Tree} {Construction} of {Large} {Datasets}},
	abstract = {Classification of large datasetsis an important data mining problem. Many classification algorithms have been proposed in the literature, but studies have shown that so far no algorithm uniformly outperforms all other algorithms in terms of quality. In this paper, we present a unifying framework for decision tree classifiers that separatesthescalability aspectsof algorithms for constructing a decision tree from the central features that determinethe quality of the tree. This generic algorithm is easyto instantiatewith specific algorithms from the literature (including C4.5, CART, CHAID, FACT,ID3 andextensions,SLIQ, Sprint and QUEST).},
	language = {en},
	author = {Gehrke, Johannes and Ramakrishnan, Raghu and Gantit, Venkatesh},
	pages = {12},
	file = {Gehrke et al. - RainForest - a Framework for Fast Decision Tree Co.pdf:/home/rp152k/Zotero/storage/WKXKBPEF/Gehrke et al. - RainForest - a Framework for Fast Decision Tree Co.pdf:application/pdf},
}

@article{shafer_sprint_nodate,
	title = {{SPRINT}: {A} {Scalable} {Parallel} {Classifier} for {Data} {Mining}},
	abstract = {Classification is an important data mining problem. Although classification is a wellstudied problem, most of the current classification algorithms require that all or a portion of the the entire dataset remain permanently in memory. This limits their suitability for mining over large databases. We present a new decision-tree-based classification algorithm, called SPRINT that removes all of the memory restrictions, and is fast and scalable. The algorithm has also been designed to be easily parallelized, allowing many processors to work together to build a single consistent model. This parallelization, also presented here, exhibits excellent scalability as well. The combination of these characteristics makes the proposed algorithm an ideal tool for data mining.},
	language = {en},
	author = {Shafer, John and Agrawal, Rakeeh and Mehta, Manish},
	pages = {12},
	file = {Shafer et al. - SPRINT A Scalable Parallel Classifier for Data Mi.pdf:/home/rp152k/Zotero/storage/CSMRSFHV/Shafer et al. - SPRINT A Scalable Parallel Classifier for Data Mi.pdf:application/pdf},
}

@article{liu_clustering_nodate,
	title = {Clustering {Via} {Decision} {Tree} {Construction}},
	language = {en},
	author = {Liu, Bing and Xia, Yiyuan and Yu, Philip S},
	pages = {25},
	file = {Liu et al. - Clustering Via Decision Tree Construction.pdf:/home/rp152k/Zotero/storage/S8RGBKQT/Liu et al. - Clustering Via Decision Tree Construction.pdf:application/pdf},
}

@article{chen_big_2020,
	title = {Big {Self}-{Supervised} {Models} are {Strong} {Semi}-{Supervised} {Learners}},
	url = {http://arxiv.org/abs/2006.10029},
	abstract = {One paradigm for learning from few labeled examples while making best use of a large amount of unlabeled data is unsupervised pretraining followed by supervised fine-tuning. Although this paradigm uses unlabeled data in a task-agnostic way, in contrast to common approaches to semi-supervised learning for computer vision, we show that it is surprisingly effective for semi-supervised learning on ImageNet. A key ingredient of our approach is the use of big (deep and wide) networks during pretraining and fine-tuning. We find that, the fewer the labels, the more this approach (task-agnostic use of unlabeled data) benefits from a bigger network. After fine-tuning, the big network can be further improved and distilled into a much smaller one with little loss in classification accuracy by using the unlabeled examples for a second time, but in a task-specific way. The proposed semi-supervised learning algorithm can be summarized in three steps: unsupervised pretraining of a big ResNet model using SimCLRv2, supervised fine-tuning on a few labeled examples, and distillation with unlabeled examples for refining and transferring the task-specific knowledge. This procedure achieves 73.9\% ImageNet top-1 accuracy with just 1\% of the labels (\${\textbackslash}le\$13 labeled images per class) using ResNet-50, a \$10{\textbackslash}times\$ improvement in label efficiency over the previous state-of-the-art. With 10\% of labels, ResNet-50 trained with our method achieves 77.5\% top-1 accuracy, outperforming standard supervised training with all of the labels.},
	urldate = {2021-05-26},
	journal = {arXiv:2006.10029 [cs, stat]},
	author = {Chen, Ting and Kornblith, Simon and Swersky, Kevin and Norouzi, Mohammad and Hinton, Geoffrey},
	month = oct,
	year = {2020},
	note = {arXiv: 2006.10029},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/home/rp152k/Zotero/storage/QMG8JMSQ/Chen et al. - 2020 - Big Self-Supervised Models are Strong Semi-Supervi.pdf:application/pdf;arXiv.org Snapshot:/home/rp152k/Zotero/storage/T5JJEDAS/2006.html:text/html},
}

@article{oord_representation_2019,
	title = {Representation {Learning} with {Contrastive} {Predictive} {Coding}},
	url = {http://arxiv.org/abs/1807.03748},
	abstract = {While supervised learning has enabled great progress in many applications, unsupervised learning has not seen such widespread adoption, and remains an important and challenging endeavor for artificial intelligence. In this work, we propose a universal unsupervised learning approach to extract useful representations from high-dimensional data, which we call Contrastive Predictive Coding. The key insight of our model is to learn such representations by predicting the future in latent space by using powerful autoregressive models. We use a probabilistic contrastive loss which induces the latent space to capture information that is maximally useful to predict future samples. It also makes the model tractable by using negative sampling. While most prior work has focused on evaluating representations for a particular modality, we demonstrate that our approach is able to learn useful representations achieving strong performance on four distinct domains: speech, images, text and reinforcement learning in 3D environments.},
	urldate = {2021-05-25},
	journal = {arXiv:1807.03748 [cs, stat]},
	author = {Oord, Aaron van den and Li, Yazhe and Vinyals, Oriol},
	month = jan,
	year = {2019},
	note = {arXiv: 1807.03748
version: 2},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/home/rp152k/Zotero/storage/CM5QM55U/Oord et al. - 2019 - Representation Learning with Contrastive Predictiv.pdf:application/pdf;arXiv.org Snapshot:/home/rp152k/Zotero/storage/IKL8KTFS/1807.html:text/html},
}

@misc{noauthor_leonardo_nodate,
	title = {Leonardo da {Vinci}},
	url = {https://www.goodreads.com/work/best_book/55861438-leonardo-da-vinci},
	abstract = {Based on thousands of pages from Leonardo's astonishing notebooks and new discoveries about his life and work, Walter Isaacson weaves a n...},
	urldate = {2021-05-24},
	keywords = {curiosity},
	file = {Snapshot:/home/rp152k/Zotero/storage/XKRHJ5QJ/34684622-leonardo-da-vinci.html:text/html},
}

@misc{noauthor_how_nodate,
	title = {How to {Take} {Smart} {Notes}: {One} {Simple} {Technique} to {Boost} {Writing}, {Learning} and {Thinking} – for {Students}, {Academics} and {Nonfiction} {Book} {Writers} by {Sönke} {Ahrens}},
	url = {https://www.goodreads.com/book/show/34507927-how-to-take-smart-notes?from_search=true&from_srp=true&qid=whieIAy7Jx&rank=1},
	urldate = {2021-05-24},
	file = {How to Take Smart Notes\: One Simple Technique to Boost Writing, Learning and Thinking – for Students, Academics and Nonfiction Book Writers by Sönke Ahrens:/home/rp152k/Zotero/storage/53SUSANB/34507927-how-to-take-smart-notes.html:text/html},
}

@misc{noauthor_mind_nodate,
	title = {The {Mind} {Illuminated}: {A} {Complete} {Meditation} {Guide} {Integrating} {Buddhist} {Wisdom} and {Brain} {Science} by {Culadasa} ({John} {Yates})},
	url = {https://www.goodreads.com/book/show/25942786-the-mind-illuminated},
	urldate = {2021-05-24},
	keywords = {meditation},
	file = {The Mind Illuminated\: A Complete Meditation Guide Integrating Buddhist Wisdom and Brain Science by Culadasa (John Yates):/home/rp152k/Zotero/storage/7XWFT5QF/25942786-the-mind-illuminated.html:text/html},
}

@article{park_contrastive_2020,
	title = {Contrastive {Learning} for {Unpaired} {Image}-to-{Image} {Translation}},
	url = {http://arxiv.org/abs/2007.15651},
	abstract = {In image-to-image translation, each patch in the output should reflect the content of the corresponding patch in the input, independent of domain. We propose a straightforward method for doing so -- maximizing mutual information between the two, using a framework based on contrastive learning. The method encourages two elements (corresponding patches) to map to a similar point in a learned feature space, relative to other elements (other patches) in the dataset, referred to as negatives. We explore several critical design choices for making contrastive learning effective in the image synthesis setting. Notably, we use a multilayer, patch-based approach, rather than operate on entire images. Furthermore, we draw negatives from within the input image itself, rather than from the rest of the dataset. We demonstrate that our framework enables one-sided translation in the unpaired image-to-image translation setting, while improving quality and reducing training time. In addition, our method can even be extended to the training setting where each "domain" is only a single image.},
	urldate = {2021-05-24},
	journal = {arXiv:2007.15651 [cs]},
	author = {Park, Taesung and Efros, Alexei A. and Zhang, Richard and Zhu, Jun-Yan},
	month = aug,
	year = {2020},
	note = {arXiv: 2007.15651},
	keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/home/rp152k/Zotero/storage/DRPQKEW6/Park et al. - 2020 - Contrastive Learning for Unpaired Image-to-Image T.pdf:application/pdf;arXiv.org Snapshot:/home/rp152k/Zotero/storage/IJYK88WN/2007.html:text/html},
}

@article{zhu_unpaired_2020,
	title = {Unpaired {Image}-to-{Image} {Translation} using {Cycle}-{Consistent} {Adversarial} {Networks}},
	url = {http://arxiv.org/abs/1703.10593},
	abstract = {Image-to-image translation is a class of vision and graphics problems where the goal is to learn the mapping between an input image and an output image using a training set of aligned image pairs. However, for many tasks, paired training data will not be available. We present an approach for learning to translate an image from a source domain \$X\$ to a target domain \$Y\$ in the absence of paired examples. Our goal is to learn a mapping \$G: X {\textbackslash}rightarrow Y\$ such that the distribution of images from \$G(X)\$ is indistinguishable from the distribution \$Y\$ using an adversarial loss. Because this mapping is highly under-constrained, we couple it with an inverse mapping \$F: Y {\textbackslash}rightarrow X\$ and introduce a cycle consistency loss to push \$F(G(X)) {\textbackslash}approx X\$ (and vice versa). Qualitative results are presented on several tasks where paired training data does not exist, including collection style transfer, object transfiguration, season transfer, photo enhancement, etc. Quantitative comparisons against several prior methods demonstrate the superiority of our approach.},
	urldate = {2021-05-23},
	journal = {arXiv:1703.10593 [cs]},
	author = {Zhu, Jun-Yan and Park, Taesung and Isola, Phillip and Efros, Alexei A.},
	month = aug,
	year = {2020},
	note = {arXiv: 1703.10593},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/home/rp152k/Zotero/storage/KSRXCALI/Zhu et al. - 2020 - Unpaired Image-to-Image Translation using Cycle-Co.pdf:application/pdf;arXiv.org Snapshot:/home/rp152k/Zotero/storage/ALARMCP8/1703.html:text/html;Full Text:/home/rp152k/Zotero/storage/RV4Q7ZFZ/Zhu et al. - 2020 - Unpaired Image-to-Image Translation using Cycle-Co.pdf:application/pdf},
}

@article{lehtinen_noise2noise_2018,
	title = {{Noise2Noise}: {Learning} {Image} {Restoration} without {Clean} {Data}},
	shorttitle = {{Noise2Noise}},
	url = {http://arxiv.org/abs/1803.04189},
	abstract = {We apply basic statistical reasoning to signal reconstruction by machine learning -- learning to map corrupted observations to clean signals -- with a simple and powerful conclusion: it is possible to learn to restore images by only looking at corrupted examples, at performance at and sometimes exceeding training using clean data, without explicit image priors or likelihood models of the corruption. In practice, we show that a single model learns photographic noise removal, denoising synthetic Monte Carlo images, and reconstruction of undersampled MRI scans -- all corrupted by different processes -- based on noisy data only.},
	urldate = {2021-05-23},
	journal = {arXiv:1803.04189 [cs, stat]},
	author = {Lehtinen, Jaakko and Munkberg, Jacob and Hasselgren, Jon and Laine, Samuli and Karras, Tero and Aittala, Miika and Aila, Timo},
	month = oct,
	year = {2018},
	note = {arXiv: 1803.04189
version: 3},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/home/rp152k/Zotero/storage/MGYCLR2N/Lehtinen et al. - 2018 - Noise2Noise Learning Image Restoration without Cl.pdf:application/pdf;arXiv.org Snapshot:/home/rp152k/Zotero/storage/WHBR4TGH/1803.html:text/html},
}

@article{zhou_w2s_2020,
	title = {{W2S}: {Microscopy} {Data} with {Joint} {Denoising} and {Super}-{Resolution} for {Widefield} to {SIM} {Mapping}},
	shorttitle = {{W2S}},
	url = {http://arxiv.org/abs/2003.05961},
	abstract = {In fluorescence microscopy live-cell imaging, there is a critical trade-off between the signal-to-noise ratio and spatial resolution on one side, and the integrity of the biological sample on the other side. To obtain clean high-resolution (HR) images, one can either use microscopy techniques, such as structured-illumination microscopy (SIM), or apply denoising and super-resolution (SR) algorithms. However, the former option requires multiple shots that can damage the samples, and although efficient deep learning based algorithms exist for the latter option, no benchmark exists to evaluate these algorithms on the joint denoising and SR (JDSR) tasks. To study JDSR on microscopy data, we propose such a novel JDSR dataset, Widefield2SIM (W2S), acquired using a conventional fluorescence widefield and SIM imaging. W2S includes 144,000 real fluorescence microscopy images, resulting in a total of 360 sets of images. A set is comprised of noisy low-resolution (LR) widefield images with different noise levels, a noise-free LR image, and a corresponding high-quality HR SIM image. W2S allows us to benchmark the combinations of 6 denoising methods and 6 SR methods. We show that state-of-the-art SR networks perform very poorly on noisy inputs. Our evaluation also reveals that applying the best denoiser in terms of reconstruction error followed by the best SR method does not necessarily yield the best final result. Both quantitative and qualitative results show that SR networks are sensitive to noise and the sequential application of denoising and SR algorithms is sub-optimal. Lastly, we demonstrate that SR networks retrained end-to-end for JDSR outperform any combination of state-of-the-art deep denoising and SR networks},
	urldate = {2021-05-23},
	journal = {arXiv:2003.05961 [cs, eess]},
	author = {Zhou, Ruofan and Helou, Majed El and Sage, Daniel and Laroche, Thierry and Seitz, Arne and Süsstrunk, Sabine},
	month = aug,
	year = {2020},
	note = {arXiv: 2003.05961},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Electrical Engineering and Systems Science - Image and Video Processing},
	file = {arXiv Fulltext PDF:/home/rp152k/Zotero/storage/462ERVYC/Zhou et al. - 2020 - W2S Microscopy Data with Joint Denoising and Supe.pdf:application/pdf;arXiv.org Snapshot:/home/rp152k/Zotero/storage/IDUJMPMT/2003.html:text/html},
}

@misc{noauthor_ixi_nodate,
	title = {{IXI} {Dataset} – {Brain} {Development}},
	url = {http://brain-development.org/ixi-dataset/},
	language = {en-US},
	urldate = {2021-05-23},
	file = {Snapshot:/home/rp152k/Zotero/storage/WHL5W3SD/ixi-dataset.html:text/html},
}

@article{zhang_poisson-gaussian_2019,
	title = {A {Poisson}-{Gaussian} {Denoising} {Dataset} with {Real} {Fluorescence} {Microscopy} {Images}},
	url = {http://arxiv.org/abs/1812.10366},
	abstract = {Fluorescence microscopy has enabled a dramatic development in modern biology. Due to its inherently weak signal, fluorescence microscopy is not only much noisier than photography, but also presented with Poisson-Gaussian noise where Poisson noise, or shot noise, is the dominating noise source. To get clean fluorescence microscopy images, it is highly desirable to have effective denoising algorithms and datasets that are specifically designed to denoise fluorescence microscopy images. While such algorithms exist, no such datasets are available. In this paper, we fill this gap by constructing a dataset - the Fluorescence Microscopy Denoising (FMD) dataset - that is dedicated to Poisson-Gaussian denoising. The dataset consists of 12,000 real fluorescence microscopy images obtained with commercial confocal, two-photon, and wide-field microscopes and representative biological samples such as cells, zebrafish, and mouse brain tissues. We use image averaging to effectively obtain ground truth images and 60,000 noisy images with different noise levels. We use this dataset to benchmark 10 representative denoising algorithms and find that deep learning methods have the best performance. To our knowledge, this is the first real microscopy image dataset for Poisson-Gaussian denoising purposes and it could be an important tool for high-quality, real-time denoising applications in biomedical research.},
	urldate = {2021-05-23},
	journal = {arXiv:1812.10366 [cs, eess, stat]},
	author = {Zhang, Yide and Zhu, Yinhao and Nichols, Evan and Wang, Qingfei and Zhang, Siyuan and Smith, Cody and Howard, Scott},
	month = apr,
	year = {2019},
	note = {arXiv: 1812.10366},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, Electrical Engineering and Systems Science - Image and Video Processing},
	file = {arXiv Fulltext PDF:/home/rp152k/Zotero/storage/Z8F3K8VP/Zhang et al. - 2019 - A Poisson-Gaussian Denoising Dataset with Real Flu.pdf:application/pdf;arXiv.org Snapshot:/home/rp152k/Zotero/storage/QV8H6RY4/1812.html:text/html},
}
