:PROPERTIES:
:ID:       a1670dc7-c7fc-45a6-8e95-1f4a6ddeb7e6
:END:
#+title: Spacy 101
#+filetags: :tool:nlp:ai:

notes when following the tutorial https://spacy.io/usage/spacy-101.
The tooling itself ([[id:68e44f89-7d87-4ac6-9c00-f6ba3c38257d][SpaCy]]) is referred throughout this zettelkasten as a sentinel subtree in the machine learning head. Once this has been processed, will also be processing https://course.spacy.io/en/ ([[id:6a8eaa08-5a8e-4696-b699-367fefb1d95b][Advanced NLP with spaCy]]) into these notes 

* Overview
** Features available
|-----------------------------+-------------------------------------------------------|
| Task/Utilisation            | Existing Relevant Node                                |
|-----------------------------+-------------------------------------------------------|
| Tokenization                | [[id:3f69fc50-5e0b-4bbd-8909-ee777434a1f5][Text Representation]]                                   |
| Part of Speech Tagging      | [[id:543414ce-fd12-470b-a38a-c61cfc10bfe4][Information Extraction]]                                |
| Dependency Parsing          | [[id:543414ce-fd12-470b-a38a-c61cfc10bfe4][Information Extraction]]                                |
| Lemmatization               | [[id:3f69fc50-5e0b-4bbd-8909-ee777434a1f5][Text Representation]]                                   |
| Sentence Boundary Detection | ---                                                   |
| Named Entity Recognition    | [[id:71a53540-e823-49a2-9049-b286ee265e62][Named Entity Recognition (NER)]]                        |
| Entity Linking              | [[id:01cef446-e7e0-48f6-af7b-d0478e689cf2][Named Entity Disambiguation and Linking (NED and NEL)]] |
| Similarity                  | see [[id:2ec4a33e-479d-466b-b2b1-0a5925c0222c][cosine similarity]] for one approach                |
| Text Classification         | [[id:f8d2207f-86d3-4501-a7bc-393fb53c52c1][Text Classification]]                                   |
| Training                    | ---                                                   |
| Serialization               | [[id:86de7485-e9c0-4b7f-9f11-adb8229afdf4][Serialization]]                                         |
|-----------------------------+-------------------------------------------------------|

** Linguistic annotations
 - variety of tooling to gain insights into the grammatical structure of text being analysed.
   - word types (part of speech), further categorization eg:- nouns into subjects and objects. same words differentiated via their POS ("google" being a verb or a noun...)

** Trained pipeline
Most of the features work independently but some require loading "trained pipelines". They're composed of the following:
 - binary weights for part of speech tagger, dependency parser, and NER
 - lexical entries : words and their attributes (spelling, length)
 - data files: lemmatization and lookup tables
 - word vectors: see [[id:3f69fc50-5e0b-4bbd-8909-ee777434a1f5][Text Representation]]
 - configs: metadata to load the pipeline with appropriate configuration
   
*** Loading a pipeline

 - trained pipelines are loaded as ~Language~ objects
 - on the first call, the pipeline will be downloaded and installed
 - these ~Language~ objects are usually named as ~nlp~
 - when passing text through a pipeline, we receive a processed ~Doc~ object

#+begin_src python
  import spacy
  nlp = spacy.load('en_core_web_sm')
  doc = nlp("the quick brown fox jumped over the lazy dog")
  for token in doc:
     print(token.text, token.pos_, token.dep_)
#+end_src

 - some convenient characteristics of a ~Doc~ object
   - is an iterable of tokens
   - each token has the attributes .text , .pos_ (part of speech) , and .dep_ (dependency relation)
   - no information is lost and all text (whitespace, unique characters) will still be accessible in the doc object
   - see https://spacy.io/api/token



 
 
* Elaborating
** Tokenization
 - see [[id:3f69fc50-5e0b-4bbd-8909-ee777434a1f5][Text Representation]]
 - segmenting text into words, punctuation and other similar discrete structures
 - this needs to be done smartly and based on context :- each period isn't a full stop ("U.S.A" for instance should be one token without any punctuations)
 - a trained pipeline (Language object usually addressed as nlp) when applied to a text, produces an iterable of tokens. 
 - first step is splitting with whitespace, after which the splits are processed from left to right with the following checks:
   1. check if it is an exception
      - "don't" => "do" and "n't"
      - "U.K." stays the same
   2. check or splittable suffixes, prefixes and infixes
      - commas, periods, hyphens or quotes are candidates for such splits
      - anti-clockwise => "anti" and "clockwise"
 - each language needs its own set of extensive hard coded data and exception rules that need to be loaded when using that particular trained pipeline.
 - for further details and customization options, see :
   - https://spacy.io/usage/linguistic-features#language-data
   - https://spacy.io/usage/linguistic-features#tokenization
** Part of speech tags and dependencies
 - post tokenization, a ~Doc~ can be parsed and tagged.
   - the statistical models come into play at this stage.
 - linguistic annotations are available as attributes of token objects.
   - strings are hashed for efficiency, so defaults are integers:
     - ~tok.pos~ : integer hash of part of speech tag
     - ~tok.pos_~ : the part of speech tag (string)
     - ~tok.dep~ : integer hash of the dependency tag
     - ~tok.dep_~ : the dependency tag as string
   - conventions for attributes:
     - base word is the integer hash
     - _ appended yields the string tag
   - a summary of the attributes of a token is as follows:
     - text : the original text
     - Lemma : base form of the word
     - POS : simple part of speech tags in the format mentioned https://universaldependencies.org/u/pos/
     - Tag : detailed part-of-speech tags
     - Dep : syntactic dependency -> relation between tokens
     - shape : capitalization, punctuation and digits (eg: Apple -> Xxxxx, U.K. -> X.X., 3 -> d)
     - is alpha : predicate on the token being composed of only alphanumeric characters
     - is stop : predicate on the token being a stop word
 - spacy.explain("...") can be used to fetch short descriptions of tags and labels
 - extra tooling : for visualizations see [[id:a784e58a-e9f9-438c-b93d-d1995a2fb184][DisplaCy]]

** Named Entities
 - a named real world object...
   - person, country, product, book title, etc..
 - a doc object is an iterable of tokens by default
 - an iterable of entities can be fetched via ~doc.ents~:

#+begin_src python
  import spacy

  # downloading and loading the languge object
  nlp = spacy.load("en_core_web_sm") 

  # creating the document object
  doc = nlp("the quick brown fox jumped over the lazy dog")

  for ent in doc.ents:
    print(ent.text, ent.start_char, ent.end_char, ent.label_)
#+end_src

 - again, the [[id:a784e58a-e9f9-438c-b93d-d1995a2fb184][DisplaCy]] visualizer 
 - for a theoretical outlook, see [[id:71a53540-e823-49a2-9049-b286ee265e62][Named Entity Recognition (NER)]]

** Word Vectors (embeddings) and Similarity
 - see [[id:3f69fc50-5e0b-4bbd-8909-ee777434a1f5][Text Representation]] and specifically [[id:06c21d6f-fa59-46b2-a8b7-c54f5f62fc78][text-embeddings]]
 - note that small trained pipelines like "en_core_web_sm" (ending in "sm") don't ship with word-vectors
   - instead, use the variant ending in "lg" : "en_core_web_lg"
 - given the pipeline ships with vectors, normal objects like ~token~, ~doc~ and ~span~ now have a ~vector~ attribute that defaults to the average of their token vectors.
#+begin_src python
  import spacy
  nlp = spacy.load("en_core_web_lg")

  doc = nlp("the quick brown fox jumps over the lazy dog")

  for token in doc:
    print(token.text, token.has_vector, token.vector_norm, token.is_oov)
#+end_src
