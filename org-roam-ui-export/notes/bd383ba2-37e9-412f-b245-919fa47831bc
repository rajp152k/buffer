* Classification Evaluation Metrics
:PROPERTIES:
:ID:       bd383ba2-37e9-412f-b245-919fa47831bc
:END:
** Primitive
- Accuracy
Percentage of correctly labelled data points
- Precision
How many positive predictions were actually positive?
- Recall
How many positives were predicted out of all the actual positives?
- F1-score/measure
Harmonic mean of Precision and Recall

Summarizing the above for binary clasifiction
#+begin_src lisp
  (defun data-generator ...)
  (defvar true-labels ...) ;; 1 - positive ; 0 - negative

  (defvar pred-labels (model (data-generator)))

  (defun count-match (true-label pred-label trues preds)
    (sum (map (lambda (actual prediction)
		(cond ((and (= actual true-label)
			    (= prediction pred-label))
		       1)
		      (t 0)))
	      trues
	      preds)))

  (defun counter (lambda (true-label pred-label)
		   (count-match true-label pred-label
				true-labels pred-labels)))

  (defvar true-positives (counter 1 1))
  (defvar true-negatives (counter 0 0))
  (defvar false-positives (counter 0 1))
  (defvar false-negatives (counter 1 0))

  (assert (= (+ true-positives true-negatives
		false-positives false-negatves)
	     (length true-labels)))

  (defvar accuracy (/ (+ true-positives true-negatives)
		      (length true-labels)))

  (defvar precision (/ true-positives
		       (+ true-positives false-positives)))

  (defvar recall (/ true-positives
		    (+ true-postives false-negatives)))

  (defun harmonic-mean (a b) (/ (* 2 a b) (+ a b)))

  (defvar f1-measure (harmonic-mean precision recall))
      #+end_src
      
** Not-so-primitive
 - Area under ROC curve
  [[https://developers.google.com/machine-learning/crash-course/classification/roc-and-auc][Classification: ROC Curve and AUC]] 
