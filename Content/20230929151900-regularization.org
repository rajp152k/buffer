:PROPERTIES:
:ID:       2f33e97a-c064-4680-9951-9fdab284eb89
:END:
#+title: Regularization
#+filetags: :ml:ai:

* Basics
** Purpose
Prevent overfitting by adding a penalty to the model's complexity.
** Techniques
Common forms of regularization include L1 (Lasso) and L2 (Ridge) regularization, which add penalties based on the absolute and squared values of model parameters, respectively.
*** L1(Lasso)
 -  leads to a sparse model with most parameters set to zero due to the nature of the penalty's contour with respect to the parameters.
 - can be used as a feature selector then: to find out which features are essential and increasing model [[id:398d134d-6193-409a-b3b5-9e7c7de86ce7][Interpretability]]
*** L2(Ridge)
 - promotes lower moduli of parameters.
 - aka weight decay
   - derivative of L2 norm is a linear absolute term that is subtracted from gradients during [[id:e419c0a9-9753-48f1-82c4-f2004cc2e29c][Stochastic Gradient Descent (SGD)]]
*** Elastic Net
 - combination of L1 and L2

*** [[id:f4c27962-ac4f-4255-a2d9-dcfe9d382daa][Batch Norm]]
*** [[id:0b7bc8b1-42e5-45d7-b7a0-d449db7895c4][Dropout]]
*** [[id:3b9a83ab-92b8-4547-8eaa-77ab36db57b8][Data Augmentation]]
** Effect
Regularization encourages the model to have smaller parameter values, reducing its complexity and preventing it from fitting noise in the training data.
** Bias-Variance Trade-off
It introduces a trade-off between fitting the training data well and having a simpler model that generalizes better to new, unseen data.
** Lambda (Hyperparameter)
The strength of regularization is controlled by a hyperparameter (lambda or alpha) that can be tuned to achieve the desired balance between bias and variance.
** Benefits
It helps improve model stability, reduce overfitting, and make models more robust in real-world scenarios.

* Specific to [[id:bc56a36d-6b62-4e9c-b540-00528d72b3b5][Neural Networks]]
** Dropout
:PROPERTIES:
:ID:       f44fd8bf-e28d-4426-a282-7813eb172cc4
:END:
 - exclude a certain percentage (or with certain probability) of the neurons (switch them off) during training during each iteration
 - the higher that hyperparameter, the more regularizing effect we introduce.
 - this can be achieved using a dropout layer
** BatchNorm
:PROPERTIES:
:ID:       b66652b4-7dbd-4441-b73a-045d780a429c
:END:
 - standardizing outputs of each layer before units of the subsequent layer receive them as input
 - results in faster and more stable training
 - not specifically a regularization technique but often exhibits regularization effects
 - no particular harm if you use it : should always use it as a rule of thumb
 - implemented as a batchNorm layer between two computational layers.
** Data Augmentation
 - usually with images
 - involves creating synthetic examples by applying various transformations on the original:
   - slight zoom in/out
   - rotating
   - flipping
   - darkening
   - and so on
** Early Stopping
:PROPERTIES:
:ID:       a46256b8-1ea5-4679-92af-4e1c5ed095f9
:END:
 - overfits due to prolonged training can result in lower validation performance
 - a common practice is to have checkpoints every certain number of epochs and store the best model so far based on validation performance
