:PROPERTIES:
:ID:       f70bec51-ce7d-404e-aa37-223f64f07691
:ROAM_ALIASES: RNN
:END:
#+title: Recurrent Neural Networks
#+filetags: :arch:ml:

* Overview

**  *Definition*:
Recurrent Neural Networks (RNNs) are a class of artificial neural networks designed to recognize patterns in [[id:0dca64bc-836f-48f4-8c48-4f9fb4d098c1][sequences]] of data, such as [[id:a8cdf417-e51e-4465-9ae1-035d9373099f][time series]] or [[id:20230713T150554.400026][natural language]].

**  *Key Features*:
  - *Sequence Handling*: RNNs possess an internal state (memory) that allows them to process sequences by maintaining a hidden state from previous inputs.
  - *Recurrent Connections*: Unlike feedforward neural networks, RNNs have loops to enable information to persist. This looping allows the network to model temporal dynamics.

**  *Architecture*:
  - *Hidden Layers*: RNNs typically consist of input, hidden, and output layers. The hidden layers have a recurrent connection with a loop that feeds back into the layer.
  - *Activation Functions*: Commonly used activation functions in RNNs include the tanh and ReLU functions. However, due to issues like vanishing gradients, other architectures like Long Short-Term Memory (LSTM) or Gated Recurrent Unit (GRU) are often used.

**  *Challenges*:
  - *[[id:5affdbd4-d042-4580-85d6-f8ade06d961c][Vanishing/Exploding Gradients]]*: Because RNNs backpropagate through time, they often face these gradient issues, where gradients may become too small or too large to manage effectively.

**  *Advancements*:
  - *[[id:4fcd41ce-1be2-4e18-8918-772c5f90690e][LSTM]] and [[id:8fa544d9-3529-484f-88a4-2000683515e2][GRU]]*: Variants of RNNs like LSTM and GRU are designed to better capture long-range dependencies without being affected significantly by gradient issues.
