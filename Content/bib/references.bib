
@misc{sheth_neurosymbolic_2023,
	title = {Neurosymbolic {AI} -- {Why}, {What}, and {How}},
	url = {http://arxiv.org/abs/2305.00813},
	doi = {10.48550/arXiv.2305.00813},
	abstract = {Humans interact with the environment using a combination of perception - transforming sensory inputs from their environment into symbols, and cognition - mapping symbols to knowledge about the environment for supporting abstraction, reasoning by analogy, and long-term planning. Human perception-inspired machine perception, in the context of AI, refers to large-scale pattern recognition from raw data using neural networks trained using self-supervised learning objectives such as next-word prediction or object recognition. On the other hand, machine cognition encompasses more complex computations, such as using knowledge of the environment to guide reasoning, analogy, and long-term planning. Humans can also control and explain their cognitive functions. This seems to require the retention of symbolic mappings from perception outputs to knowledge about their environment. For example, humans can follow and explain the guidelines and safety constraints driving their decision-making in safety-critical applications such as healthcare, criminal justice, and autonomous driving. This article introduces the rapidly emerging paradigm of Neurosymbolic AI combines neural networks and knowledge-guided symbolic approaches to create more capable and flexible AI systems. These systems have immense potential to advance both algorithm-level (e.g., abstraction, analogy, reasoning) and application-level (e.g., explainable and safety-constrained decision-making) capabilities of AI systems.},
	urldate = {2024-06-17},
	publisher = {arXiv},
	author = {Sheth, Amit and Roy, Kaushik and Gaur, Manas},
	month = may,
	year = {2023},
	note = {arXiv:2305.00813 [cs]},
	keywords = {Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:/home/rp152k/Zotero/storage/XKHCL4IW/Sheth et al. - 2023 - Neurosymbolic AI -- Why, What, and How.pdf:application/pdf;arXiv.org Snapshot:/home/rp152k/Zotero/storage/JWNHUYEJ/2305.html:text/html},
}

@misc{garcez_neurosymbolic_2020,
	title = {Neurosymbolic {AI}: {The} 3rd {Wave}},
	shorttitle = {Neurosymbolic {AI}},
	url = {http://arxiv.org/abs/2012.05876},
	doi = {10.48550/arXiv.2012.05876},
	abstract = {Current advances in Artificial Intelligence (AI) and Machine Learning (ML) have achieved unprecedented impact across research communities and industry. Nevertheless, concerns about trust, safety, interpretability and accountability of AI were raised by influential thinkers. Many have identified the need for well-founded knowledge representation and reasoning to be integrated with deep learning and for sound explainability. Neural-symbolic computing has been an active area of research for many years seeking to bring together robust learning in neural networks with reasoning and explainability via symbolic representations for network models. In this paper, we relate recent and early research results in neurosymbolic AI with the objective of identifying the key ingredients of the next wave of AI systems. We focus on research that integrates in a principled way neural network-based learning with symbolic knowledge representation and logical reasoning. The insights provided by 20 years of neural-symbolic computing are shown to shed new light onto the increasingly prominent role of trust, safety, interpretability and accountability of AI. We also identify promising directions and challenges for the next decade of AI research from the perspective of neural-symbolic systems.},
	urldate = {2024-06-17},
	publisher = {arXiv},
	author = {Garcez, Artur d'Avila and Lamb, Luis C.},
	month = dec,
	year = {2020},
	note = {arXiv:2012.05876 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, I.2.4, I.2.6},
	file = {arXiv Fulltext PDF:/home/rp152k/Zotero/storage/62RGF8PB/Garcez and Lamb - 2020 - Neurosymbolic AI The 3rd Wave.pdf:application/pdf;arXiv.org Snapshot:/home/rp152k/Zotero/storage/3QTEEPKH/2012.html:text/html},
}

@article{hitzler_neural-symbolic_2020,
	title = {Neural-symbolic integration and the {Semantic} {Web}},
	volume = {11},
	issn = {22104968, 15700844},
	url = {https://www.medra.org/servlet/aliasResolver?alias=iospress&doi=10.3233/SW-190368},
	doi = {10.3233/SW-190368},
	language = {en},
	number = {1},
	urldate = {2024-06-17},
	journal = {Semantic Web},
	author = {Hitzler, Pascal and Bianchi, Federico and Ebrahimi, Monireh and Sarker, Md Kamruzzaman},
	editor = {Janowicz, Krzysztof},
	month = jan,
	year = {2020},
	pages = {3--11},
	file = {Hitzler et al. - 2020 - Neural-symbolic integration and the Semantic Web.pdf:/home/rp152k/Zotero/storage/TXQTYC6N/Hitzler et al. - 2020 - Neural-symbolic integration and the Semantic Web.pdf:application/pdf},
}

@misc{bottou_machine_2011,
	title = {From {Machine} {Learning} to {Machine} {Reasoning}},
	url = {http://arxiv.org/abs/1102.1808},
	doi = {10.48550/arXiv.1102.1808},
	abstract = {A plausible definition of "reasoning" could be "algebraically manipulating previously acquired knowledge in order to answer a new question". This definition covers first-order logical inference or probabilistic inference. It also includes much simpler manipulations commonly used to build large learning systems. For instance, we can build an optical character recognition system by first training a character segmenter, an isolated character recognizer, and a language model, using appropriate labeled training sets. Adequately concatenating these modules and fine tuning the resulting system can be viewed as an algebraic operation in a space of models. The resulting model answers a new question, that is, converting the image of a text page into a computer readable text. This observation suggests a conceptual continuity between algebraically rich inference systems, such as logical or probabilistic inference, and simple manipulations, such as the mere concatenation of trainable learning systems. Therefore, instead of trying to bridge the gap between machine learning systems and sophisticated "all-purpose" inference mechanisms, we can instead algebraically enrich the set of manipulations applicable to training systems, and build reasoning capabilities from the ground up.},
	urldate = {2024-06-17},
	publisher = {arXiv},
	author = {Bottou, Leon},
	month = feb,
	year = {2011},
	note = {arXiv:1102.1808 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/home/rp152k/Zotero/storage/MJ6VVSW2/Bottou - 2011 - From Machine Learning to Machine Reasoning.pdf:application/pdf;arXiv.org Snapshot:/home/rp152k/Zotero/storage/GYZQ2VD2/1102.html:text/html},
}

@misc{de_raedt_statistical_2020,
	title = {From {Statistical} {Relational} to {Neuro}-{Symbolic} {Artificial} {Intelligence}},
	url = {http://arxiv.org/abs/2003.08316},
	doi = {10.48550/arXiv.2003.08316},
	abstract = {Neuro-symbolic and statistical relational artificial intelligence both integrate frameworks for learning with logical reasoning. This survey identifies several parallels across seven different dimensions between these two fields. These cannot only be used to characterize and position neuro-symbolic artificial intelligence approaches but also to identify a number of directions for further research.},
	urldate = {2024-06-17},
	publisher = {arXiv},
	author = {De Raedt, Luc and Dumančić, Sebastijan and Manhaeve, Robin and Marra, Giuseppe},
	month = mar,
	year = {2020},
	note = {arXiv:2003.08316 [cs]},
	keywords = {Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:/home/rp152k/Zotero/storage/U4FGFF77/De Raedt et al. - 2020 - From Statistical Relational to Neuro-Symbolic Arti.pdf:application/pdf;arXiv.org Snapshot:/home/rp152k/Zotero/storage/8IVPLMHU/2003.html:text/html},
}

@misc{noauthor_neuro-symbolic_nodate,
	title = {Neuro-{Symbolic} {Artificial} {Intelligence} - workshops},
	url = {https://people.cs.ksu.edu/~hitzler/nesy/},
	urldate = {2024-06-17},
	file = {Neuro-Symbolic Artificial Intelligence:/home/rp152k/Zotero/storage/63DNPJFW/nesy.html:text/html},
}

@misc{lamb_graph_2021,
	title = {Graph {Neural} {Networks} {Meet} {Neural}-{Symbolic} {Computing}: {A} {Survey} and {Perspective}},
	shorttitle = {Graph {Neural} {Networks} {Meet} {Neural}-{Symbolic} {Computing}},
	url = {http://arxiv.org/abs/2003.00330},
	doi = {10.48550/arXiv.2003.00330},
	abstract = {Neural-symbolic computing has now become the subject of interest of both academic and industry research laboratories. Graph Neural Networks (GNN) have been widely used in relational and symbolic domains, with widespread application of GNNs in combinatorial optimization, constraint satisfaction, relational reasoning and other scientific domains. The need for improved explainability, interpretability and trust of AI systems in general demands principled methodologies, as suggested by neural-symbolic computing. In this paper, we review the state-of-the-art on the use of GNNs as a model of neural-symbolic computing. This includes the application of GNNs in several domains as well as its relationship to current developments in neural-symbolic computing.},
	urldate = {2024-06-17},
	publisher = {arXiv},
	author = {Lamb, Luis C. and Garcez, Artur and Gori, Marco and Prates, Marcelo and Avelar, Pedro and Vardi, Moshe},
	month = jun,
	year = {2021},
	note = {arXiv:2003.00330 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Computation and Language, Computer Science - Logic in Computer Science},
	file = {arXiv Fulltext PDF:/home/rp152k/Zotero/storage/2SP4FU4E/Lamb et al. - 2021 - Graph Neural Networks Meet Neural-Symbolic Computi.pdf:application/pdf;arXiv.org Snapshot:/home/rp152k/Zotero/storage/URE2YKP9/2003.html:text/html},
}

@misc{battaglia_relational_2018,
	title = {Relational inductive biases, deep learning, and graph networks},
	url = {http://arxiv.org/abs/1806.01261},
	doi = {10.48550/arXiv.1806.01261},
	abstract = {Artificial intelligence (AI) has undergone a renaissance recently, making major progress in key domains such as vision, language, control, and decision-making. This has been due, in part, to cheap data and cheap compute resources, which have fit the natural strengths of deep learning. However, many defining characteristics of human intelligence, which developed under much different pressures, remain out of reach for current approaches. In particular, generalizing beyond one's experiences--a hallmark of human intelligence from infancy--remains a formidable challenge for modern AI. The following is part position paper, part review, and part unification. We argue that combinatorial generalization must be a top priority for AI to achieve human-like abilities, and that structured representations and computations are key to realizing this objective. Just as biology uses nature and nurture cooperatively, we reject the false choice between "hand-engineering" and "end-to-end" learning, and instead advocate for an approach which benefits from their complementary strengths. We explore how using relational inductive biases within deep learning architectures can facilitate learning about entities, relations, and rules for composing them. We present a new building block for the AI toolkit with a strong relational inductive bias--the graph network--which generalizes and extends various approaches for neural networks that operate on graphs, and provides a straightforward interface for manipulating structured knowledge and producing structured behaviors. We discuss how graph networks can support relational reasoning and combinatorial generalization, laying the foundation for more sophisticated, interpretable, and flexible patterns of reasoning. As a companion to this paper, we have released an open-source software library for building graph networks, with demonstrations of how to use them in practice.},
	urldate = {2024-06-17},
	publisher = {arXiv},
	author = {Battaglia, Peter W. and Hamrick, Jessica B. and Bapst, Victor and Sanchez-Gonzalez, Alvaro and Zambaldi, Vinicius and Malinowski, Mateusz and Tacchetti, Andrea and Raposo, David and Santoro, Adam and Faulkner, Ryan and Gulcehre, Caglar and Song, Francis and Ballard, Andrew and Gilmer, Justin and Dahl, George and Vaswani, Ashish and Allen, Kelsey and Nash, Charles and Langston, Victoria and Dyer, Chris and Heess, Nicolas and Wierstra, Daan and Kohli, Pushmeet and Botvinick, Matt and Vinyals, Oriol and Li, Yujia and Pascanu, Razvan},
	month = oct,
	year = {2018},
	note = {arXiv:1806.01261 [cs, stat]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/home/rp152k/Zotero/storage/S6TRQ9KR/Battaglia et al. - 2018 - Relational inductive biases, deep learning, and gr.pdf:application/pdf;arXiv.org Snapshot:/home/rp152k/Zotero/storage/JRHFDMQ4/1806.html:text/html},
}

@misc{yi_neural-symbolic_2019,
	title = {Neural-{Symbolic} {VQA}: {Disentangling} {Reasoning} from {Vision} and {Language} {Understanding}},
	shorttitle = {Neural-{Symbolic} {VQA}},
	url = {http://arxiv.org/abs/1810.02338},
	doi = {10.48550/arXiv.1810.02338},
	abstract = {We marry two powerful ideas: deep representation learning for visual recognition and language understanding, and symbolic program execution for reasoning. Our neural-symbolic visual question answering (NS-VQA) system first recovers a structural scene representation from the image and a program trace from the question. It then executes the program on the scene representation to obtain an answer. Incorporating symbolic structure as prior knowledge offers three unique advantages. First, executing programs on a symbolic space is more robust to long program traces; our model can solve complex reasoning tasks better, achieving an accuracy of 99.8\% on the CLEVR dataset. Second, the model is more data- and memory-efficient: it performs well after learning on a small number of training data; it can also encode an image into a compact representation, requiring less storage than existing methods for offline question answering. Third, symbolic program execution offers full transparency to the reasoning process; we are thus able to interpret and diagnose each execution step.},
	urldate = {2024-06-17},
	publisher = {arXiv},
	author = {Yi, Kexin and Wu, Jiajun and Gan, Chuang and Torralba, Antonio and Kohli, Pushmeet and Tenenbaum, Joshua B.},
	month = jan,
	year = {2019},
	note = {arXiv:1810.02338 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/home/rp152k/Zotero/storage/WQ8SJ528/Yi et al. - 2019 - Neural-Symbolic VQA Disentangling Reasoning from .pdf:application/pdf;arXiv.org Snapshot:/home/rp152k/Zotero/storage/9CIDLP4A/1810.html:text/html},
}

@misc{vaswani_attention_2023,
	title = {Attention {Is} {All} {You} {Need}},
	url = {http://arxiv.org/abs/1706.03762},
	doi = {10.48550/arXiv.1706.03762},
	abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
	urldate = {2024-06-19},
	publisher = {arXiv},
	author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
	month = aug,
	year = {2023},
	note = {arXiv:1706.03762 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:/home/rp152k/Zotero/storage/335MFALG/Vaswani et al. - 2023 - Attention Is All You Need.pdf:application/pdf;arXiv.org Snapshot:/home/rp152k/Zotero/storage/KR7F69YM/1706.html:text/html},
}

@misc{zhang_new_2023,
	title = {A {New} {Information} {Theory} of {Certainty} for {Machine} {Learning}},
	url = {http://arxiv.org/abs/2304.12833},
	doi = {10.48550/arXiv.2304.12833},
	abstract = {Claude Shannon coined entropy to quantify the uncertainty of a random distribution for communication coding theory. We observe that the uncertainty nature of entropy also limits its direct usage in mathematical modeling. Therefore we propose a new concept troenpy,as the canonical dual of entropy, to quantify the certainty of the underlying distribution. We demonstrate two applications in machine learning. The first is for the classical document classification, we develop a troenpy based weighting scheme to leverage the document class label. The second is a self-troenpy weighting scheme for sequential data and show that it can be easily included in neural network based language models and achieve dramatic perplexity reduction. We also define quantum troenpy as the dual of the Von Neumann entropy to quantify the certainty of quantum systems.},
	urldate = {2024-07-07},
	publisher = {arXiv},
	author = {Zhang, Arthur Jun},
	month = apr,
	year = {2023},
	note = {arXiv:2304.12833 [cs, math]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Computation and Language, Computer Science - Information Retrieval, Computer Science - Information Theory},
	file = {arXiv Fulltext PDF:/home/rp152k/Zotero/storage/RREK5VQY/Zhang - 2023 - A New Information Theory of Certainty for Machine .pdf:application/pdf;arXiv.org Snapshot:/home/rp152k/Zotero/storage/MS3Y5DB3/2304.html:text/html},
}

@misc{qiao_we-math_2024,
	title = {We-{Math}: {Does} {Your} {Large} {Multimodal} {Model} {Achieve} {Human}-like {Mathematical} {Reasoning}?},
	shorttitle = {We-{Math}},
	url = {http://arxiv.org/abs/2407.01284},
	doi = {10.48550/arXiv.2407.01284},
	abstract = {Visual mathematical reasoning, as a fundamental visual reasoning ability, has received widespread attention from the Large Multimodal Models (LMMs) community. Existing benchmarks, such as MathVista and MathVerse, focus more on the result-oriented performance but neglect the underlying principles in knowledge acquisition and generalization. Inspired by human-like mathematical reasoning, we introduce WE-MATH, the first benchmark specifically designed to explore the problem-solving principles beyond end-to-end performance. We meticulously collect and categorize 6.5K visual math problems, spanning 67 hierarchical knowledge concepts and five layers of knowledge granularity. We decompose composite problems into sub-problems according to the required knowledge concepts and introduce a novel four-dimensional metric, namely Insufficient Knowledge (IK), Inadequate Generalization (IG), Complete Mastery (CM), and Rote Memorization (RM), to hierarchically assess inherent issues in LMMs' reasoning process. With WE-MATH, we conduct a thorough evaluation of existing LMMs in visual mathematical reasoning and reveal a negative correlation between solving steps and problem-specific performance. We confirm the IK issue of LMMs can be effectively improved via knowledge augmentation strategies. More notably, the primary challenge of GPT-4o has significantly transitioned from IK to IG, establishing it as the first LMM advancing towards the knowledge generalization stage. In contrast, other LMMs exhibit a marked inclination towards Rote Memorization - they correctly solve composite problems involving multiple knowledge concepts yet fail to answer sub-problems. We anticipate that WE-MATH will open new pathways for advancements in visual mathematical reasoning for LMMs. The WE-MATH data and evaluation code are available at https://github.com/We-Math/We-Math.},
	urldate = {2024-07-07},
	publisher = {arXiv},
	author = {Qiao, Runqi and Tan, Qiuna and Dong, Guanting and Wu, Minhui and Sun, Chong and Song, Xiaoshuai and GongQue, Zhuoma and Lei, Shanglin and Wei, Zhe and Zhang, Miaoxuan and Qiao, Runfeng and Zhang, Yifan and Zong, Xiao and Xu, Yida and Diao, Muxi and Bao, Zhimin and Li, Chen and Zhang, Honggang},
	month = jul,
	year = {2024},
	note = {arXiv:2407.01284 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Symbolic Computation},
	file = {arXiv Fulltext PDF:/home/rp152k/Zotero/storage/IU5B49TG/Qiao et al. - 2024 - We-Math Does Your Large Multimodal Model Achieve .pdf:application/pdf;arXiv.org Snapshot:/home/rp152k/Zotero/storage/NSFPNXAV/2407.html:text/html},
}

@misc{petryk_importance_2024,
	title = {On the {Importance} of {Reproducibility} of {Experimental} {Results} {Especially} in the {Domain} of {Security}},
	url = {http://arxiv.org/abs/2407.06760},
	doi = {10.1109/MECO62516.2024.10577919},
	abstract = {Security especially in the fields of IoT, industrial automation and critical infrastructure is paramount nowadays and a hot research topic. In order to ensure confidence in research results they need to be reproducible. In the past we reported [18] that in many publications important information such as details about the equipment used are missing. In this paper we report on our own experiments that we run to verify the parameters reported in the datasheets that came along with our experimental equipment. Our results show that there are significant discrepancies between the datasheets and the real world data. These deviations concern accuracy of positions, movements, duration of laser shots etc. In order to improve reproducibility of results we therefore argue on the one hand that research groups verify the data given in datasheets of equipment they use and on the other hand that they provide measurement set-up parameters in globally accepted units such as cm, seconds, etc.},
	urldate = {2024-07-10},
	author = {Petryk, Dmytro and Kabin, Ievgen and Langendörfer, Peter and Dyka, Zoya},
	month = jul,
	year = {2024},
	note = {arXiv:2407.06760 [cs]},
	keywords = {Computer Science - Cryptography and Security, Computer Science - Hardware Architecture},
	file = {arXiv Fulltext PDF:/home/rp152k/Zotero/storage/HJHIXPCN/Petryk et al. - 2024 - On the Importance of Reproducibility of Experiment.pdf:application/pdf;arXiv.org Snapshot:/home/rp152k/Zotero/storage/6BBHTLMP/2407.html:text/html},
}
