:PROPERTIES:
:ID:       0fb8c9c4-f491-4d40-b6b7-a6a331316c01
:END:
#+title: Classification
#+filetags: :task:ai:

#+begin_center
To classify given data points into one or more known data classes.
#+end_center

* OverArching types
- binary classification
- multiclass classification
- multilabel classification (multiple viable labels for a data point)

* Generic Classification Pipeline
:PROPERTIES:
:ID:       b5bbb126-c808-468c-962d-8361aa8c8dd1
:END:
1. Obtain datum, label pairs that can be used for learning.
2. split the dataset into train, validation (optional testing) parts and decide on the [[id:bd383ba2-37e9-412f-b245-919fa47831bc][evaluation metrics]] that will be employed
3. pre-process the splits accordingly and proceed with alternating training/validation phase. Degrees of freedom that can be explored
   - improving feature engineering 
   - tuning the model hyperparameters
4. test and benchmark the model on the test set using the evaluation metric.
5. deployment : on new data points with unknown categories

* Classification Evaluation Metrics
:PROPERTIES:
:ID:       bd383ba2-37e9-412f-b245-919fa47831bc
:END:
** Primitive
- Accuracy
Percentage of correctly labelled data points
 - weighted accuracy according to classes of importance is a viable modification
- Precision
How many positive predictions were actually positive?
- Recall
How many positives were predicted out of all the actual positives?
- F1-score/measure
Harmonic mean of Precision and Recall

Summarizing the above for binary clasifiction
#+begin_src lisp
  (defun data-generator ...)
  (defvar true-labels ...) ;; 1 - positive ; 0 - negative

  (defvar pred-labels (model (data-generator)))

  (defun count-match (true-label pred-label trues preds)
    (sum (map (lambda (actual prediction)
		(cond ((and (= actual true-label)
			    (= prediction pred-label))
		       1)
		      (t 0)))
	      trues
	      preds)))

  (defun counter (lambda (true-label pred-label)
		   (count-match true-label pred-label
				true-labels pred-labels)))

  (defvar true-positives (counter 1 1))
  (defvar true-negatives (counter 0 0))
  (defvar false-positives (counter 0 1))
  (defvar false-negatives (counter 1 0))

  (assert (= (+ true-positives true-negatives
		false-positives false-negatves)
	     (length true-labels)))

  (defvar accuracy (/ (+ true-positives true-negatives)
		      (length true-labels)))

  (defvar precision (/ true-positives
		       (+ true-positives false-positives)))

  (defvar recall (/ true-positives
		    (+ true-postives false-negatives)))

  (defun harmonic-mean (a b) (/ (* 2 a b) (+ a b)))

  (defvar f1-measure (harmonic-mean precision recall))
      #+end_src
      
** Not-so-primitive
*** Area under ROC curve
  [[https://developers.google.com/machine-learning/crash-course/classification/roc-and-auc][Classification: ROC Curve and AUC]] 
**** ROC Curve (Receiver Operating Characteristic)
- X and Y Axes: The ROC curve is a plot with the False Positive Rate (FPR) on the X-axis and the True Positive Rate (TPR) on the Y-axis.
  - False Positive Rate (FPR): It's the ratio of false positives (incorrectly classified as positive) to the total number of actual negatives.
  - True Positive Rate (TPR): It's the ratio of true positives (correctly classified as positive) to the total number of actual positives.

- Thresholds: The ROC curve is created by varying the classification threshold of a binary classifier. At different threshold values, the model classifies data as positive or negative.

- Curve Shape: The curve starts at the bottom-left corner (0,0) and goes towards the top-right corner (1,1). A diagonal line (the "random guess" line) would represent a model that's no better than random guessing.

- Performance: A model's performance is determined by how far its ROC curve is from the random guess line. The closer it is to the top-left corner, the better the model's ability to distinguish between classes.

**** AUC (Area Under the Curve)
- AUC Value: The AUC is a single number that quantifies the overall performance of the model based on the ROC curve. It's the area under the ROC curve.
- Interpretation: AUC values range from 0 to 1. A model with an AUC of 0.5 represents random guessing (no discrimination), while a model with an AUC of 1.0 represents perfect discrimination.
- High AUC: A higher AUC indicates that the model is good at distinguishing between the two classes. It suggests that, on average, the model ranks positive examples higher than negative examples.

**** Summary
The ROC curve is a graphical representation of a model's ability to classify data, showing the trade-off between false positives and true positives at different thresholds. The AUC summarizes this performance in a single number, with a higher AUC indicating better discrimination ability. It's a common tool for evaluating the performance of binary classification models. 
* Classifiers
:PROPERTIES:
:ID:       31a028e3-f87a-4aae-85b7-04bc0c8a32af
:END:
** Types
*** Generative Classifiers
 - model probablility of observing a data point's feature set given the label and report the argmax
*** Discriminative Classifiers
 - model the joint probability distribution of the feature-label set report the one with the max probabilty 
** Examples
*** Naive Bayes
 - naive usage of the bayes theorem.
 - prediction is the class which has the highest likelihood given the current data point.
 - the distribution to evaluate the above likelihood is built from the dataset.
 - example of a generative classifier
*** Logistic Regression
 - example of discriminative classifier* Classification Evaluation Metrics
 - slap on a logistic function on top of a regressor
 - serves as a quick baseline (see [[id:8c6bce48-0cac-487c-8789-e08f22c00094][MVP]])
*** Support Vector Machine
 - tries finding a separation hyperplane post mapping(via a kernel function) data points to a higher dimensional space
 - unlike logistic regression, can deal with non-linear boundaries
 - can take longer to train
*** Deep Learning based
 - don't use a hammer when pliers get it done elegantly.
 - The usage is reduced down to the formulation of the feature set and labels in a format that's compatible with deep learning algorithms
    - see [[id:20230713T110040.814546][Deep Learning]]
    - relevant architectures : CNNs, RNNs and more complex variants
 - [[id:64c6a881-ef47-4973-a821-34e0cc085f34][Transfer Learning]] methods are increasingly more feasible today: finetuning a generically pretrained large neural network can produce good results fairly quickly.
   - see [[id:4252684e-4148-442a-838c-d8c3e842be42][Deep Learning in NLP]] in for specific info
* Possible Problems
 -  [[id:89c8e59e-e058-4edc-bd85-b4db9eb089a9][Class Imbalance]]
 -  Feature Engineering
   - too sparse representations (in case of text)
   - un-normalized/ un-standardized numerical features
   - too many linearly dependent numerical features that could be represented by a single amalgamation and help reduce the model complexity.
 -  Hyperparameter Tuning
    - model/algorithm dependent
* Relevant Nodes
 - [[id:f8d2207f-86d3-4501-a7bc-393fb53c52c1][Text Classification]]
 - [[id:91729987-32db-482a-bc1b-91469579413b][Logistic Regression]]
 

