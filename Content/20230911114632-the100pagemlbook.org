:PROPERTIES:
:ID:       33882050-cc96-447b-9022-bcbb1757af9d
:END:
#+title: the100pagemlbook
#+filetags: :book:ml:ai:

The parent reference sentinel for this book is rooted as [[id:523db378-6e64-41a3-8890-ad782c67b5e9][The Hundred Page Machine Learning Book]] under the major machine learning node.
I populate this node with the intention to index into other major nodes of the field and fill in some holes that are generic and require a book  for an end to end coverage cause tending to them in a non-project oriented scenario isn't worth the time - this isn't a book summary.

* Introduction
** What is ML?
 - subdomain of [[id:6f9a4752-aa66-42cf-9b88-2e4fa2091511][Computer Science]]
   - building algorithms that rely on a collection of instances about a phenomenon to solve tasks related to it. 
   - these instances can be sourced in multiple ways:
     - from nature
     - from another algorithm
     - synthesized according to human behaviour
 - a very basic breakdown for the process of machine learning is broken down as :
   1. Gather a dataset
   2. algorithmically build a statistical model based on that dataset
** Types of ML
 - [[id:90bcd50c-a360-4fd2-a5f2-356a6c7035cd][Supervised Learning]]
 - [[id:322ac854-7baf-41e2-8895-c33b2ef08f91][Semi-Supervised Learning]]
 - [[id:fded2ca7-e60a-4c83-842f-bc60f1ea5260][Unsupervised Learning]]
 - [[id:9cac188e-8229-4c7a-9cb4-eeb5e81f8010][Reinforcement Learning]]
** PAC learning
 - on learnability and understanding relationship between model error, training set size, model building time and the mathematical formulation of the model
 - this is an index; explored in depth in a dedicated node
   - [[id:7eadb2fb-1568-441b-a97a-99bd4ab7be0a][PAC learning]]
* Notations and Definitions
 - well, I won't be focusing on the notation cause I'm proceeding with symbolic representations via s-expressions
 - still indexing the important definitions though
** Scalars, Vectors and Sets 
 - [[id:7517a8cb-763d-40fd-8355-ad7ff8aca8e8][Scalar]]
 - [[id:691ea9d3-1311-49be-b198-f9b10dac441d][Vector]]
 - [[id:c1a12380-9aad-4969-8b6a-cfceebfa984f][Sets]]

** Math as S-expressions
 - I won't be using mathmatical notation in these notes cause of the reasons listed here : [[id:6efc5118-aa6d-43f7-bd46-5f0460819813][NIL]] , and somewhere in the [[id:1729][Index]].
 - I find it convenient to just build up an abstraction when needed

** [[id:a31671c6-12ea-4fc9-93cb-73d29fd508a6][Functions]]
 - mathematically speaking : a relation that maps elements from the domain [[id:c1a12380-9aad-4969-8b6a-cfceebfa984f][set]] to the codomain set (aka range), with some constraints on the nature of this map.
 - manipulation of the various parameters in the above dfeinition (types of the sets, nature of the mapping, etc) gives rise to different classes of functions.
** Max and Arg Max
 - The former refers to the maximum element in a set
   - this requires the notion of comparison to be defined over that set

 - The latter refers to an optimal element in a set that pushes the value of a function to its optimal value (max) 

 - Note that both are functions and have relevant counterparts as min and arg min
** Derivatives and Gradient
 - a complete treatment will be present in [[id:b9a1ec54-7977-418f-9181-8c4ff0254aed][Matrix Calculus]]
** [[id:7e1c9ba8-d8e8-43e3-bb83-0e8c4ea1442e][Random Variables]] 
 - helps capture the notion of probability into mathematical expressions
   - can be discrete and continuous
 - [[id:91b6fb5d-6447-43fe-8412-2054bb79979a][Probability]] is a large epistemological root in itself and will be explored independently
 - a few important terms relevant to a random variable that will be referred to going forward:
   - probability mass distribution, probability density
   - expectation
   - variance, covariance

 - these will be conceptualized programmatically going forward
   
** [[id:aa2716de-6052-42e9-bfb7-1483a768c1e4][Unbiased Estimator]]
 - if the expectation of the estimator random variable is equal to the statistic being estimated, it's termed as an unbiased estimator of that statistic.
 - for instance, sample mean is an unbiased estimator of the mean statistic of a random variable

** Bayes' Rule (aka Bayes' Theorem)
 - denotes equivalence of the probability of a joint event broken down into its conditionals and respective singulars

#+begin_src lisp
  (defmacro P (event)
    ...) ; probabilty of a given event

  (defmacro and (X-sample Y-sample)
    ...) ;conceptualizes the event (X-sample and Y-sample)
  (defmacro given (X-sample Y-sample)
    ...) ; conceptualizes the event (X-sample, given Y-sample)

  (defmacro sample (rand-var instance)
    ...) ; denotes the event that instance was sampled from rand-var

  (defmacro declare-randvar (tag ...)
    ...) ; declare a random variable with associated info

  ;;Bayes' rule then is:

  (declare-randvar X ...)
  (declare-randvar Y ...)

  (let ((X=x (sample X x))
	(Y=y (sample Y y)))
    (assert-mathematical-equal
     (P (and X=x
	     Y=y))
     (* (P (given X=x
		  Y=y))
	(P Y=y))
     (* (P (given Y=y
		  X=x))
	(P X=x))))
#+end_src

** [[id:74fbb1e0-a63f-48b1-8b3f-072270a0a1b1][Parameter Estimation]]
 - to be explored in depth in the main node
** [[id:0fb8c9c4-f491-4d40-b6b7-a6a331316c01][Classification]] vs [[id:93082142-64cf-45b2-9878-f3a96f596ccf][Regression]] 
** Model-Based vs Instance Based Learning
 - Model Based : use training data to build a model with learned parameters - see [[id:b278fc18-a6cf-4e41-b015-502dbad9f056][Support Vector Machine]]
 - Instance Based : use the whole dataset as a model (no intermediate parameters) -  see [[id:b8194cd8-57bc-4f4a-9862-baa8d5599033][k-Nearest Neighbors]]
** Shallow vs [[id:20230713T110040.814546][Deep Learning]]
 - Shallow learning : model parameters are learned directly from features of training samples
 - Deep Learning uses optimization techniques for a loss function and it's more of a black box than the shallow learning alternative
   
* Fundamental Algorithms
** [[id:4459e764-2e05-4941-ba61-06b9bb2b9e08][Linear Regression]]
** [[id:91729987-32db-482a-bc1b-91469579413b][Logistic Regression]]
** [[id:a2c424a5-d412-496c-abcb-1fd216548a02][Decision Trees]]
** [[id:b8194cd8-57bc-4f4a-9862-baa8d5599033][k-Nearest Neighbors]]
* Anatomy of a Learning Algorithm
Any learning algorithm is centered around certain basics:
 - A [[id:d99d5a5f-93fc-4f3b-b72e-ea59037956f9][loss function]]
 - an [[id:7b9be887-8c39-4a37-8217-f0e21a6cb64e][optimization]] ..
   - criterion, inspired from the loss function
   - routine, that finds a solution to the optimization criterion
* Basic Practice 
** [[id:5ca10a46-d9b8-4a6b-8aab-34ec17d55049][Feature Engineering]]
** [[id:c3e62ed9-31d6-4ceb-ad82-c4d0e9b48c77][Algorithm Selection]]
** Three sets
 - training set
 - validation set
 - testing set

Keeping the validation and test set the same size is the common practice. A usual split for a traditional situation would be 70%:15%:15%.
With big data though, it's reasonable to go for 95%:2.5%:2.5%.

briefly speaking, the individual purposes are as follows:
 - fitting the algorithm on the training set
 - checking performance intermittently on the validation set and making considerable hyperparametric choices like which algorithm to use and tuning the particular algorithm.
 - evaluating performance on the test set.
** Underfitting and Overfitting
*** Bias (Underfitting)
:PROPERTIES:
:ID:       9b9f920e-31a4-40b8-b051-fc01b5e4a4ac
:END:
Bias refers to the error introduced by approximating a real-world problem, which may be complex, by a simplified model. A model with high bias pays little attention to the training data and oversimplifies the problem. This leads to underfitting, where the model doesn't capture the underlying patterns in the data and performs poorly on both the training and unseen data. In simpler terms, a high-bias model is too simple to represent the data.

*** Variance (Overfitting)
:PROPERTIES:
:ID:       a5c9934b-ff07-48da-bcc7-86c0894f783f
:END:
Variance, on the other hand, refers to the error introduced due to the model's sensitivity to small fluctuations or noise in the training data. A model with high variance is overly complex and captures noise in the training data rather than the underlying patterns. This leads to overfitting, where the model performs very well on the training data but poorly on unseen data because it has essentially memorized the training data and doesn't generalize well.

*** Summary
- High bias (underfitting) results from a model that is too simple and doesn't fit the data well.
- High variance (overfitting) results from a model that is too complex and fits the training data too closely.

The goal in machine learning is to strike a balance between bias and variance, finding a model that is complex enough to capture the essential patterns in the data but not so complex that it overfits and fails to generalize to new, unseen data. This balance is crucial for building models that perform well in real-world scenarios. Techniques like cross-validation and regularization are often used to help find this balance.

** [[id:2f33e97a-c064-4680-9951-9fdab284eb89][Regularization]]
** Model Performance assesment
already covered before in [[id:bd383ba2-37e9-412f-b245-919fa47831bc][Classification Evaluation Metrics]]
For regression, using a loss function is the most basic choice to rate performance.
To compare the metric obtained, we need a ..
*** Baseline
 - to compare any metric, consider building a baseline model for the task at hand
   - for regression this can be as simple as building a mean-model, for instance (one that predicts the average of all, for all)
   - for classification, you could predict the mode or build a simple model.

** Hyperparameter Tuning
 - if choices are low, perform a grid search over a validation set.
   - for numerical hyperparameters with large ranges, perform multiple searches:
     1. first with a large span using a logarithmic scale
     2. then use linear scales when you get an idea about what ranges are worth paying attention to


This can be time consuming and other efficient techniques like random search and [[id:60d8aa20-8b45-4216-9211-c47354d421bf][Bayesian Hyperparameter Optimization]] exist.
     
** [[id:121fd726-d347-4eab-b434-4b9f22a713fa][Cross validation]]
 - when the dataset isn't large enough to carve form a separate validation set, consider k-fold cross validation:
   1. partition the dataset into k folds
   2. at each fitting iteration:
      - fit on k-1 folds
      - validate on one
      - change the validation fold for the next iteration
   3. for reporting the final validation metric, use an average of the k computations


Finally, once fit on all these fold combinations, the model performance is ready to be evaluated on the test set.
     
* [[id:bc56a36d-6b62-4e9c-b540-00528d72b3b5][Neural Networks]] and [[id:20230713T110040.814546][Deep Learning]]
