:PROPERTIES:
:ID:       20230713T110040.814546
:END:
#+title: Deep Learning
#+filetags: :ml:ai:

* Base
** What does it mean to go deep?
Reference to working with [[id:bc56a36d-6b62-4e9c-b540-00528d72b3b5][Neural Networks]] with more than two intermediate layers.
Some staple problems one experiences during back-propogation as we deepen our networks are:
**** exploding gradients
These are easier to deal with as one still has the notion of direction retained although the magnitudes are large.
Techniques such as gradient clipping and [[id:2f33e97a-c064-4680-9951-9fdab284eb89][Regularization]] should help with this.
**** vanishing gradients
When backpropogating, when employing the chain rule, the corresponding gradients for the earlier parameters may diminish in magnitude due to the nature of the intermediate layers' gradients' natures. The tanh has gradients between 0 and 1 for instance, backpropogating n layers back - this will result in exponentially diminishing layers, reulting in virtually no learning for the initial layers.
Some common techniques used to deal with this issue are:
 - ReLU activations are more robust against diminishing gradients
 - Skip connections, like in [[id:325569c7-0b2f-478c-8792-e63702d4b1b6][Residual Neural Networks]], also helps deal with the issue by skipping layers

*** Present situation
The above issues with deep networks have been addressed satisfactorily and networks with 100s of hidden layers (a non output and non input layer) can be successfully trained.

** Applications of DL
*** [[id:20230713T150554.400026][NLP]]
 - QnA
 - Speech recog
 - summarization
 - classifying docs
*** [[id:2e6d0401-1bce-4aa8-8b5b-9a0f5557f15b][Computer Vision]]
 - Satellite and Drone Imagery Interpretation
 - Face Recognition
 - Image Captioning
 - Reading Traffic Signs
 - autonomous driving
*** [[id:f36c3afa-b266-42da-9fdd-fa12fbee4147][Medicine]]
 - Anomaly detection (in radiology, CT, MRI and X-ray for instance)
 - detecting features in pathology slides
 - measuring features in ultrasounds
 - diagnosing diabetic retinopathy
*** [[id:20230809T042424.883127][Biology]]
 - Folding, classifying, ... proteins
 - genomic tasks
 - cell classification
 - analysing protein/protein interaction
*** Image Generation
 - colorization
 - upscaling resolution
 - denoising
 - stylistic adaptation
*** Recommendation Systems
 - web search
 - product recommendations
 - landing page layouts
*** [[id:a765de0f-c74c-4753-9aa4-363654301e52][Games]]
 - Chess
 - Go
 - complex RTS
*** [[id:f1ec552e-a7c4-47ae-9dd2-a23733d1da92][Robotics]]
 - handling objects that are challenging to locate (shiny, unusual texture etc)

** [[id:17d3a745-72b6-4cf7-a0a2-ed5ff69830bf][Training Loop]]
** towards SOTA: squeezing out performance in DL
This section (inspired by [[id:c6e31908-5622-4e17-9ccd-6b4e71f53ff1][DL for Coders : fastai + pytorch]], written with Computer Vision in mind), logs tweaks and techniques that are usually employed to squueze out performance from a Deep Learning model. 

Do note that this section is about training a model from scratch, without employing [[id:64c6a881-ef47-4973-a821-34e0cc085f34][Transfer Learning]] or doing so only in cases of a very distinct domain where the pretraining task isn't too closely related to the desired task.

*** Data Related
**** Protoypal datasets
Whenever training a model from scratch, it isn't necessary to conduct all experiments on the given dataset right away. Consider creating a prototypal dataset that retains the major characteristics of the original dataset and allows for a lower experiment duration allowing for more iterations.
Once we are done testing out some hypotheses on the toy dataset, proceed with the tests on the original dataset.
Don't prefer testing out novel ideas directly on a larger dataset where the time to feedback is too large.
**** Presizing
 - Data Augmentation during batch transform involve several processes that might lead to a lower quality image even at the same resolution.
   - this occurs due to the transforms needed interpolation computations that might not exactly preserve quality.
 - A simple trick to deal with this is before the transforms are applied, apply a large presizing image transform (larger than the final resizing).
   - also combine all the augmentation transforms into one to be applied on a whole batch on the GPU instead of performing them as item transforms calling for multiple interpolation computations. 
 - This should help achieve the desired transforms without much qualitative loss.
 - read more in Chapter 5 of [[id:c6e31908-5622-4e17-9ccd-6b4e71f53ff1][DL for Coders : fastai + pytorch]]
**** Normalizing
 - Normalizing to the same stats that the base model was trained with is important when using transfer learning
 - but it's also important to normalize batches when training from scratch
   - whenever using fundamental pretrained models, do look for the normalization stats as well. Without them, the model sees tensor distributions that it wasn't trained on (very far away from what it was intended for (not just a transfer learning scenario))
**** Progressive Resizing
 - from C7 of [[id:c6e31908-5622-4e17-9ccd-6b4e71f53ff1][DL for Coders : fastai + pytorch]]
 - the initial layers of a model learn coarse features (edges, corners etc) whereas later layers might learn to identify finer ones (patterns, explicit features from the dataset)..
 - training on lower res images for a few cycles followed by progressively increasing resolution of images each cycle (fit first, fine_tune on later cycles when described through the perspective of a deep learning library and not transfer learning).
 - initial cycles should be faster to train and tune the initial layers
 - later cycles can focus on learning the finer features in layers down the model
 - is a data augmentation strategy
 - might not be useful when transfer learning when pretraining dataset was of similar image sizes though : as the weights have already been iterated upon enough; do try out when the transfer of task is related to generalizing to images of different sizes
**** Test Time Augmentation 
 - instead of a single center crop (or similar transforms) that affect the contents of the image, crop at multiple locations, process them and average or sensibly ensemble the individual predictions
**** Mixup
 - train on linear combination of 2 images and their one-hot encoded corresponding targets
 - train with tradition cross entropy loss as before
 - intends to make data augmentation an independent process from the domain specific expertise of the dataset
 - checkout https://arxiv.org/abs/1710.09412
   - do read up on the issues addressed and further caused as well
     - low likelihood of overfitting
     - new targets will not always be binary now : observe and comment on effects of the same
 - reserve for use when can train for a large number of epochs
**** Label Smoothing
  - involves moving away from the one-hot encoded targets
  - deals with overfitting issues
  - is a way to avoid with overconfidence with predictions
  - check out : https://arxiv.org/abs/1512.00567
  - all 0s are set to eps/N and the 1 is set to 1-eps+(eps/N)
    - they do add to 1
    - eps is a hyperparameter (usually 0.1)
    - N is total number of classes
* References
*** DL for Coders : fastai + pytorch
:PROPERTIES:
:ID:       c6e31908-5622-4e17-9ccd-6b4e71f53ff1
:END:
- https://course.fast.ai/Resources/book.html
- upgrading skills : specializing further in fast ai and pytorch
- will populate notes in here in accordance with what I learn there
- will also be coding along in python in org-babel cells for comprehensive pass of the book
  
