:PROPERTIES:
:ID:       739d8493-d7a6-4eee-b31a-44d087f4fb42
:END:
#+title: cloud native
#+filetags: :arch:programming:cloud:

Cloud native refers to designing, developing, and operating applications that optimize cloud computing platforms' features and capabilities.

 I plan to extensively explore the ecosystem and understand the potential trajectory of modern computing and its future:
  - https://landscape.cncf.io/.
  - https://landscape.cncf.io/guide#introduction

 The Projects sub-node in here hosts an index into tooling that I've enqueued for exploration.

 All the major practical components of the cloud native landscape (as of 0x22AB) are being developed in [[id:ad4ba668-b2ec-47b1-9214-2284aedaceba][Golang]] : initializing exploration of the same directed towards my intentions of contributing competently in the CNCF landscape.

* The Path to Cloud Native
- The primary incentive so far pushing the evolution of computing services has been the pressure to [[id:56dbce77-b258-4fde-a6c7-f865e476c879][scale]].
- 1950s : about the [[id:b72d3ca3-53fa-4a97-964f-cbc1a8d612a4][mainframe]] computer with all the [[id:a8cbf516-055a-4ef7-9afe-7a780bda52ab][logic]] and [[id:d45dae92-5148-4220-b8dd-e4da80674053][data]] residing together as one [[id:5be3075a-d718-4f44-b031-4df5547423a2][monolith]].
- 1980s : [[id:a4e712e1-a233-4173-91fa-4e145bd68769][networks]] of personal computers encouraged some changes. Some application logic could be off-loaded to these PCs (partioning into the [[id:e944d11b-ba53-4dc1-aee9-3793f59be8ac][Client-Server Architecture]]) : the first major move towards decoupled services.
- 1990s : the dotcom rush led to explosion of (SaaS) [[id:cbcb26f4-dd24-4f59-8003-25573a7cd034][Software as a Service]].  The associated logistics of operating (develop, deploy, maintain) such a service began to get complex. This further encouraged the decoupling of the business layer in the Client Server architecture into multiple [[id:54978664-78a5-4c2c-ae33-c4e6a14d6bb0][Microservices]].
- 2000s : AWS popularized (IaaS) [[id:de6e9e57-6ba8-4d37-8e62-1a2c2327b275][Infrastructure as a Service]] : yielding the initiation of the term [[id:bc1cc0cf-5e6a-4fee-b9a5-16533730020a][Cloud Computing]].
* Important Terms (checkout [[id:314236f7-81ae-48b7-b62b-dc822119180e][System Design]])
** [[id:56dbce77-b258-4fde-a6c7-f865e476c879][Scalability]]
** [[id:adaf5bfa-48f9-415b-893e-7398b10f383e][Loose Coupling]]
** [[id:b24fb743-99bb-4e1a-b4a4-3b81c9677360][Resilience]]
** [[id:2cd51b23-f253-40e2-8c5d-6f2924ca484d][Manageability]]
** [[id:3913909e-2b8d-465c-8303-5c634bd08f57][Observability]]
* [[id:2b29d5fb-58ab-41fe-a8b3-59b9ee675b5f][Fallacies of Distributed Computing]]
* Cloud Native Patterns
- sourced from chapter 4 of [[id:64bfc13e-1b7c-4cbe-ba0e-9d17ebaacef1][BOOK: Cloud Native Go]]
** context.Context
- read up : https://pkg.go.dev/context
- used to idiomatically convey, cancellation signals, deadline abstractions (timeouts), etc.
- context values are thread safe
- read up : https://dave.cheney.net/2017/01/26/context-is-for-cancelation
- also read up : https://dave.cheney.net/2017/08/20/context-isnt-for-cancellation
** Stability Patterns
*** Circuit Breaker
:PROPERTIES:
:ID:       64b6bd2a-de67-4f28-9406-336879845d80
:END:
- https://en.wikipedia.org/wiki/Circuit_breaker_design_pattern
- automatically degrades service functions in response to a likely fault, preventing larger or cascading failures by eliminating recurring errors and providing reasonable error responses
- 2 major components :
  - Circuit
  - Breaker
- Consider the Scenario:->
  - User Request <-> Business Logic Layer <-> Data Layer
  - the Business Logic implements a Breaker if the Circuit (Data Layer in this case has failed)
  - will retry after sensible pauses and backoff strats
  - won't continue requesting if failures are assured.
**** Code


- Consider the Circuit type as, that signifies the signature of the function that's interacting with the database. An error as one of its returns is necessary.

  #+begin_src go
type Circuit func(context.Context) (string, error)
  #+end_src

- Consider the Breaker function that accepts a function of type Circuit and some failure threshold information (number of consecutive failures to be tolerated in this case, before breaking (opening) the circuit), returning a wrapped circuit with the same function signature.

  #+begin_src go

func Breaker(circuit Circuit, failureThreshold uint) Circuit {

	consecutiveFailures := 0
	lastAttempt := time.Now()

	// for closures access over breaker's administrative data
	// failure count and retry times
	var m sync.RWMutex

	return func(context.Context) (string,error) {
		//see exponential backoff
		m.RLock()

		d := consecutiveFailures - int(failureThreshold)

		if d >= 0 { //circuit breaking in case of failure
			shouldRetryAt := lastAttempt.Add(time.Second*2 << d)
			if !time.Now().After(shouldRetryAt){
				m.Runlock()
				return "", errors.New("Service unreachable")
			}
		}

		m.Runlock()

		// continuing request if no track of failures

		response, err := circuit(ctx)

		m.Lock() // acquiring write locks for closure's commons
		defer m.Unlock()

		lastAttempt = time.Now()

		if err != nil {
			consecutiveFailures++
			return response, err
		}

		consecutiveFailures = 0
		return response, nil
	}
}

  #+end_src

*** Debounce
:PROPERTIES:
:ID:       d78e2fbe-8c51-489c-b97c-74b01a0abcb6
:END:
- etymological origins in electronic circuits : https://www.geeksforgeeks.org/switch-debounce-in-digital-circuits/
- limits the frequency of a function invocation so that only the first or last in a cluster of calls is actually performed.
- is native to javascript but can port to others as needed, will be proceeding in golang
- 2 components:
  - Circuit : the computation to be regulated
  - Debounce : A closure over Circuit that manages the calls
- similar logic to [[id:64b6bd2a-de67-4f28-9406-336879845d80][Circuit Breaker]]s in that the closure maintains the rate limiting logic and state
**** Code
- on each call of the Debounce returned closure, regardless of the outcome, a time interval is set.
  - calls before expiry of that duration are ignored, any after the duration are passed along to the inner Circuit function.
    - this is a "function-first" : i.e cache results and ignore the latter calls
  - Alternatively, a "funciton-last" implementation will accumulate a series of requests before calling Circuit
    - This could be useful when the inner circuit needs some kick-starting corpus of inputs (think [[id:e0b818a4-972a-43e7-922f-e3e7a47af4d1][autocompletion]])
    - can be employed if the response can be delayed a little and increased latency is not an issue.
- The Core Circuit can be a function as follows

  #+begin_src go
type Circuit func(context.Context) (string, error)
  #+end_src

- The Debounce prepped closure can then be structured as follows (function-first)

  #+begin_src go
func DebounceFirst(circuit Circuit, d time.Duration) Circuit {

	var threshold time.Time
	var result string // result cache
	var err error
	var m sync.Mutex

	return func(ctx context.Context) (string, error) {
		m.Lock()

		defer func() {
			threshold = time.Now().Add(d)
			m.Unlock()
		}()

		if time.Now().Before(threshold){
			//return cached result before threshold
			return result, err
		}

		// if expired, compute and cache result
		// in the enclosed variable result
		result, err = circuit(ctx)

		return result, err
	}
}

  #+end_src

  - a function-last implementation needs a little more book-keeping

    #+begin_src go
func DebounceLast(circuit Circuit, d time.Duration) Circuit {
        var threshold time.Time = time.Now()
        var ticker *time.Ticker
        var result string
        var err error
        var once sync.Once
        var m sync.Mutex
        return func(ctx context.Context) (string, error) {
                m.Lock()
                defer m.Unlock()
                threshold = time.Now().Add(d)
                once.Do(func() {
                        ticker = time.NewTicker(time.Millisecond * 100)
                        go func() {
                                defer func() {
                                        m.Lock()
                                        ticker.Stop()
                                        once = sync.Once{}
                                        m.Unlock()
                                }()
                                for {
                                        select {
                                        case <-ticker.C:
                                                m.Lock()
                                                if time.Now().After(threshold) {
                                                        result, err = circuit(ctx)
                                                        m.Unlock()
                                                        return
                                                }
                                                m.Unlock()
                                        case <-ctx.Done():
                                                m.Lock()
                                                result, err = "", ctx.Err()
                                                m.Unlock()
                                                return
                                        }
                                }
                        }()
                })
                return result, err
        }
}
    #+end_src

    #+RESULTS:

*** Retry
:PROPERTIES:
:ID:       e5870690-91ef-41f4-adea-eb48c3be2325
:END:
- https://learn.microsoft.com/en-us/azure/architecture/patterns/retry
- accounts for a possible transient fault in a distributed system by transparently retrying a failed operation
- 2 components:
  - Effector : interacts with the service
  - Retry : accepts the effector, returning a closure over it
**** Code
- like the Circuit, the effector will have a function signature as follows
#+begin_src go
type Effector func(context.Context) (string, error)
#+end_src

- the Retry can take in paramters like the number of retries and delay durations, returning a closure of the same signature as the Effector

  #+begin_src go
func Retry(effector Effector, retries int, delay time.Duration) Effector{
	return func(ctx context.Context) (string, error) {
		for r := 0; ; r++ {
			response, err := effector(ctx)
			if err == nil || r >= retries {
				return response, err
			}
			log.Printf("Attempt %d failed; retrying in %v", r + 1, delay)

			select {
			case <- time.After(delay):
			case <- ctx.Done():
				return "", ctx.Err()
			}
		}
	}
}
  #+end_src

  - emulating an erroneous function to try out retry

    #+begin_src go
var count int

func EmulateTransientError(ctx context.Context) (string, error) {
	count++

	if count <= 3 {
		return "intentional fail", errors.New("error")
	} else {
		return "success", nil
	}
}

func main() {
	r := Retry(EmulateTransientError, 5, 2*time.Second)

	res, err := r(context.Background())

	fmt.Println(res,err)
}
    #+
*** Throttle
:PROPERTIES:
:ID:       f437c67e-a680-4400-8640-1fd32cc9e363
:END:
- limits the frequency of a funciton call to some maximum number of invocations per minute
- See [[id:a9f836f0-d43d-4e97-96fc-06f75e982d15][Rate Limiting]] Algorithms
- diff w/ [[id:d78e2fbe-8c51-489c-b97c-74b01a0abcb6][Debounce]]
  - debounce collates clusters of calls (across flexible durations) into representative boundary calls
  - throttle limits the amount of calls in a relatively fixed duration
- 2 components
  - Effector : the function being regulated
  - Throttle : the enwrapping closure over Effector : implementing the rate limiting layer

    #+begin_src  go
type Effector func(context.Context) (string, error)
    #+end_src

    #+begin_src go
func Throttle(e Effector, max uint, refill uint, d time.Duration) Effector {
	var tokens = max
	var once sync.Once

	return func(ctx context.Context) (string, error) {
		if ctx.Err() != nil {
			return "", ctx.Err()
		}
	}

	once.Do(func() {
		ticker := time.NewTicker(d)

		go func() {
			defer ticker.Stop()

			for {
				select {
				case <-ctx.Done():
					return

				case <- ticker.C:
					t := tokens + refill
					if t > max {
						t = max
					}
					tokens = t
				}
			}
		}()
	})

	if tokens <= 0 {
		return "", fmt.Errorf("too many calls")
	}

	tokens--

	return e(ctx)
}
    #+end_src

*** Timeout
:PROPERTIES:
:ID:       f58d7534-8fef-4af0-bf8f-45cf50375e93
:END:
- allows a process to stop waiting for an answer once it's clear that an answer may not be coming
- 3 components:
  - client : calls a slow function
  - slowfunction : a long running function
  - timeout : wrapper over slow function
- straightforward if a function utilizes context.Context in golang,
  #+begin_src go
ctx := context.Background()
ctxt,cancel := context.WithTimeout(ctx, 10*time.Second)
defer cancel()
result, err := SomeFunction(ctxt)
  #+end_src
- This isn't usually the case though
  - build a closure that respects the context in such a case, followed by a select over your injected timeout context and the result of a goroutined slow function
- will need to convert the slow function into a context respecting wrapper  as follows

#+begin_src go
type SlowFunction func(string) (string, error)

type WithContext func(context.Context, string) (string, error)

func Timeout(f SlowFunction) WithContext {
	return func(ctx context.Context, arg string) (string error) {
		chres := make(chan string) //channel for results
		cherr := make(chan string) //channel for errors

		go func() {
			res, err := f(arg) //dispatch slow function
			chres <- res
			cherr <- err
		}()

		select {
		case res := <-chres://if done before timeout
			return res, <-cherr
		case <-ctx.Done()://in case of timeout
			return "",ctx.Err()
		}
	}
}
#+end_src

- finally, using Timeout will be like
#+begin_src go
func main() {
	ctx := context.Background()
	ctxt, cancel := context.WithTimeout(ctx, 1*time.Second)
	defer cancel()

	timeout := Timeout(Slow)
	res, err := timeout(ctxt, "some input")

	fmt.Println(res, err)
}
#+end_src

- an alternative to using context.Context (context.Context is the preferred method btw), is using the time.After function : https://pkg.go.dev/time#After
** [[id:618d0535-411d-4c36-b176-84413ec8bfc1][Concurrency]] Patterns
* CNCF (Cloud Native Computing Foundation)
:PROPERTIES:
:ID:       56e931a4-16af-4eba-bcd0-c8f0b9566153
:END:

The CNCF is a vendor-neutral open source community that fosters the adoption and advancement of cloud-native technologies. It defines cloud-native as:

- [[id:d4627a77-fafc-4c76-91a2-59a84e42de71][Container]]-packaged: Software is packaged as container images.

- Dynamically orchestrated: Containers are managed by [[id:f822f8f6-89eb-4aa8-ac8f-fdcff3f06fb9][orchestrators]] like [[id:c2072565-787a-4cea-9894-60fad254f61d][Kubernetes]].

- [[id:54978664-78a5-4c2c-ae33-c4e6a14d6bb0][Microservices]]-oriented: Applications are composed of loosely coupled, modular services.

- [[id:58ea31e4-95ae-4c25-b475-c8686fe23817][Automated]]: Infrastructures and pipelines are managed declaratively and automated.

** Canonical Layers to Cloud-Native
**** Provisioning
**** Runtime
**** Orchestration and Management
**** App Definition and Development
** Ancillary Pillars to Cloud-Native
**** Observability and Analysis
**** Platform
** Projects
*** [[id:58ea31e4-95ae-4c25-b475-c8686fe23817][Devops]]
|------------+--------------------------|
| Project    | Utility                  |
|------------+--------------------------|
| [[id:3ea6cf40-f57e-4fa8-b451-6ccb0928249b][Argo]]       | CI/CD                    |
| [[id:60e66e66-d394-42e7-a624-303ddb295395][Flux]]       | CI/CD                    |
| [[id:28387722-d0ca-4c0b-8436-5c4c2ae4ba13][Helm]]       | App. Def. & Image Builds |
| [[id:c50c85ac-0b21-486f-99ac-00f325e2c43c][KEDA]]       | AutoScaling              |
| [[id:c2072565-787a-4cea-9894-60fad254f61d][Kubernetes]] | Orchestration            |
|------------+--------------------------|
*** Compute
|------------+-------------------|
| Project    | Utility           |
|------------+-------------------|
| [[id:2e953b92-6b7c-46ce-aa31-f278601ab005][Containerd]] | Container Runtime |
| [[id:320016f3-53be-4756-b9b8-3a89714ff1d9][cri-o]]      | Container Runtime |
|------------+-------------------|
*** Storage
|---------+---------------------------|
| Project | Utility                   |
|---------+---------------------------|
| [[id:204e8cda-5566-4828-9df8-ac362878ec3c][Rook]]    | Storage                   |
| [[id:63dbe80a-9503-470f-94f7-af72190f35db][TiKV]]    | distributed key-val store |
| [[id:731a8c2c-da51-419b-8224-774913c597e6][Vitess]]  | Scalable Relational  DB   |
|---------+---------------------------|
*** [[id:a4e712e1-a233-4173-91fa-4e145bd68769][Networking]]
|---------+----------------------|
| Project | Utility              |
|---------+----------------------|
| [[id:52877388-0a18-497b-afe4-3bea87d3db68][Cilium]]  | Cloud Native Network |
| [[id:d3f2c59a-602d-4a88-8828-82797f25fbd3][CoreDNS]] | Service Discovery    |
| [[id:3568f42c-6e48-4d10-8249-c95c080a975c][etcd]]    | Service Discovery    |
| [[id:97574f3d-86a1-4491-9cdd-5428d658103b][Envoy]]   | Service Proxy        |
| [[id:2f89e95e-9bfa-4762-a5f1-b1caae1f364b][lstio]]   | Service Mesh         |
| [[id:bb7b9374-8773-45f5-992d-c969d7cbded2][LinkerD]] | Service Mesh         |
|---------+----------------------|
*** Security
|-------------------+------------------------|
| Project           | Utility                |
|-------------------+------------------------|
| [[id:03338f6b-47ff-4ad2-b666-d981dc062844][Falco]]             | security & compliance  |
| [[id:13806880-ed30-4b21-8c67-016da5037ce7][Open Policy Agent]] | security & compliance  |
| [[id:b976e243-b69d-42f2-9ffa-dbd7ac8469c4][Spiffe]]            | Key Management         |
| [[id:c96bc90b-0f7e-4943-803e-916fcd036c19][Spire]]             | Key Management         |
| [[id:a1a26817-85fe-481b-b41e-659502094361][TUF]]               | Update System Security |
|-------------------+------------------------|
*** Meta
|------------+---------------------|
| Project    | Utility             |
|------------+---------------------|
| [[id:b656ae66-8364-4803-8262-41aa3cb18061][fluentd]]    | Logging             |
| [[id:eb789bb7-2fb4-43dc-86a1-6297a7654cee][Harbor]]     | Container Registry  |
| [[id:0c8ba7f9-e7c8-4c79-89b6-bb0c9992d1bd][Jaeger]]     | Distributed Tracing |
| [[id:55a62ff7-7160-4e6e-9bb5-0df996bf995e][Prometheus]] | Monitoring & Alerts |
|------------+---------------------|
